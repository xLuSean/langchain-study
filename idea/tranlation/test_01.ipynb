{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal is translate technical artical, but not technical terms\n",
    "\n",
    "#### idea:\n",
    "1. COT\n",
    "2. pydantic model: native_translate, revised_translate\n",
    "\n",
    "The idea is to have multi-steps, to improve the qulity of the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Literal\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    native_translation: str = Field(..., description=\"The native translation of the text\")\n",
    "    revised_translation: str = Field(..., description=\"The revised translation of the text\")\n",
    "\n",
    "LanguageType = Literal[\"English\", \"Japanese\", \"Chinese\"]\n",
    "\n",
    "class TranslatePayload(OutputSchema):\n",
    "    source_language: LanguageType = Field(..., description=\"The original language of the text\")\n",
    "    target_language: LanguageType = Field(..., description=\"The target language for the translation of the text\")\n",
    "\n",
    "    @field_validator('target_language')\n",
    "    def validate_target_language(cls, value, info):\n",
    "        source_language = info.data.get('source_language')\n",
    "        if source_language and value == source_language:\n",
    "            raise ValueError(\"Target language should be different from the source language\")\n",
    "        return value\n",
    "\n",
    "class TranslatePayloads(BaseModel):\n",
    "    payloads: List[TranslatePayload]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "translate_output_parser = JsonOutputParser(pydantic_object=OutputSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPrompt:\n",
    "    SYSTEM_PROMPT = \"\"\"Your job is to translate the data provide by a human user from {source_language} to {target_language}, focusing on translating as much as possible while carefully preserving technical context.\n",
    "\n",
    "1.Proprietary Names & Jargon: Keep all brand names, product names, technical jargon, and acronyms in their original form. ex [EDR(Endpoint Detection and Response), SWG(Secure Web Gateway), TCP/IP(Transmission Control Protocol/Internet Protocol), PM(Product Manager), AI(Artificial Intelligence)]\n",
    "2.Units of Measurement: Do not translate numerical units (e.g., GB, MB/s, GHz).\n",
    "3.Code & Command-Line: Leave any code snippets, command-line instructions, or programming syntax unchanged.\n",
    "4.Abbreviations & Short Forms: Keep all technical abbreviations (e.g., API, TCP/IP) without translation.\n",
    "5.Consistency: Make sure technical accuracy is prioritized over natural fluency, especially for industry-specific terms.\n",
    "6.Boolean Values & Data Types: Do not translate Boolean values (“True”/“False”) or NoneType (“None”).\n",
    "7.Technical Context: For any ambiguous technical terms, keep the original text in parentheses for reference.\n",
    "\n",
    "Try to translate the data as much as possible, but follow the above guidelines to ensure technical accuracy.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "You will first generate a native_translation of the text, and then revise it to a revised_translation, ensure technical accuracy and context preservation.\n",
    "\n",
    "Please ensure the output is formatted as specified, in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "    HUMAN_PROMPT = \"\"\"\n",
    "{article}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = TranslationPrompt.SYSTEM_PROMPT\n",
    "human_prompt = TranslationPrompt.HUMAN_PROMPT\n",
    "\n",
    "translation_prompt_tempalte = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                template=system_prompt,\n",
    "                input_variables=[\"source_language\", \"target_language\", \"article\"],\n",
    "                partial_variables={\"format_instructions\": translate_output_parser.get_format_instructions()}\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = translation_prompt_tempalte.invoke({\"source_language\": \"English\", \"target_language\": \"Japanese\", \"article\": \"This is a test article\"})\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "translation_chain = translation_prompt_tempalte | llm | translate_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article = \"Artificial intelligence (AI) has rapidly evolved in recent years, becoming an integral part of various industries. From healthcare to finance, AI is revolutionizing how tasks are performed, improving efficiency and accuracy. In healthcare, AI is used for predictive diagnostics and personalized treatment plans, enabling doctors to provide better patient care. In the financial sector, AI algorithms help detect fraud and make data-driven investment decisions. As AI continues to advance, ethical considerations such as privacy and job displacement become increasingly important. Balancing innovation with these concerns will be key to harnessing AI’s full potential.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_2 = \"\"\"\n",
    "In today’s threat landscape, cybersecurity pivots around concepts like EDR, SIEM, and SOC orchestration. EDR (Endpoint Detection and Response) focuses on real-time telemetry, leveraging XDR frameworks to aggregate data from diverse endpoints, thus providing heightened threat visibility. SIEM (Security Information and Event Management) platforms, like Splunk and ArcSight, enable SOC (Security Operations Center) teams to ingest, parse, and correlate logs, streamlining MTTR (Mean Time to Respond).\n",
    "\n",
    "Attack vectors such as APTs (Advanced Persistent Threats), spear-phishing, and MITM (Man-in-the-Middle) attacks exploit vulnerabilities within an organization’s perimeter and layered defenses. Red and Blue Teams simulate these TTPs (Tactics, Techniques, and Procedures) to test and harden cybersecurity posture. MFA (Multi-Factor Authentication) and IAM (Identity and Access Management) remain pivotal in restricting unauthorized access.\n",
    "\n",
    "The Zero Trust model, focusing on “never trust, always verify,” is crucial in modern architectures, especially with SASE (Secure Access Service Edge) deployments, integrating SWG (Secure Web Gateway) and CASB (Cloud Access Security Broker) functionalities. Encryption protocols, like TLS 1.3 and AES-256, enforce data confidentiality across untrusted networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translation_chain.invoke({\"source_language\": \"English\", \"target_language\": \"Chinese\", \"article\": test_article})\n",
    "for key , value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translation_chain.invoke({\"source_language\": \"English\", \"target_language\": \"Chinese\", \"article\": test_article_2})\n",
    "for key , value in result.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    print(\"======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
