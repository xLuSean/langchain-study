{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref: https://github.com/sakunaharinda/ragatouille-book/blob/main/book/2_Query_Transformation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation\n",
    "\n",
    "The main idea behind the Query Transformation is that translate/transform the user query in a way that the LLM can correctly answer the question. For instance, if the user asks an ambiguous question, our RAG retriever might retrieve incorrect (or ambiguous) documents based on the embeddings that are not very relevant to answer the user question, leading the LLM to hallucinate answers. There are few ways to tackle this problem. Some of them are,\n",
    "\n",
    "- [Step-back prompting](https://arxiv.org/pdf/2310.06117): This involves encouraging the LLM to take a step back from a given question or problem and pose a more abstract, higher-level question that encompasses the essence of the original inquiry.\n",
    "- [Least-to-most prompting](https://arxiv.org/pdf/2205.10625): This allows to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems.\n",
    "- Query re-writing ([Multi-Query](https://medium.com/@kbdhunga/advanced-rag-multi-query-retriever-approach-ad8cd0ea0f5b) or [RAG Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)): This allows to generate multiple questions from the original question with different wording and perspectives. Then retrieve documents using the similarity scores between each question and the vector store to answer the orginal question.\n",
    "\n",
    "A blog post about query transformation by Langchain can be found [here](https://blog.langchain.dev/query-transformations/). \n",
    "\n",
    "Now, let's try to implement the above techniques using LangChain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Introduction notebook, we first import the libraries, load documents, split them, generate embeddings, store them in a vector store and create the retriever using the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.load import loads, dumps\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean_liu/miniconda3/envs/langchain/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# embedding = OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Query\n",
    "\n",
    "In multi-query approach, we first use an LLM (here it is an instance of GPT-4) to generate 5 different questions based on our original question. To do that, we create a prompt and encapsulate it with the `ChatPromptTemplate`. Then we create the chain using LCEL, to read the user input and assign it to the `question` placeholder of the prompt, send the prompt to the LLM, parse the output containing 5 questions seperated by new line charcters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 5 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newline `\\n`.\n",
    "    \n",
    "    Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether or not our query generation works by invoking the created chain with a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What factors should be taken into account when assessing LLM-generated content using another LLM?  \\nHow can we effectively evaluate the output of one LLM using another LLM?  \\nWhat considerations are important for evaluating LLM outputs with a different LLM model?  \\nWhat criteria should guide the evaluation of LLM generations through the lens of another LLM?  \\nWhat aspects must be addressed when employing LLMs to critique the generation of other LLMs?  ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the 5 questions, we parallelly retrieve the most relevant 5 documents for each question (resulting in a list of lists) and create a new document list by taking the unique documents of the union of all the retrieved documents. To do that we create another chain, `retrieval_chain` using LCEL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_union(docs: List[List]):\n",
    "    all_docs = [dumps(d) for doc in docs for d in doc]\n",
    "    unique_docs = list(set(all_docs))\n",
    "    \n",
    "    return [loads(doc).page_content for doc in unique_docs] # We only return page contents\n",
    "\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | get_context_union\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, Canada:\\nAssociation for Computational Linguistics.',\n",
       " 'ample, The evaluation pipeline of Vicuna (Zheng\\net al., 2023) has gained significant interest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare candidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear how reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs (Dong et al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  [cs.CL]  30 Aug 2023',\n",
       " 'with human intent (He et al., 2023). While hu-\\nman evaluation is treated as the most accurate mea-\\nsurement of model performance, it is costly and\\ntime-consuming to operate at scales. Consider-\\ning the potent capabilities of LLMs, researchers\\nhave started utilizing LLMs to evaluate the profi-\\nciency of generative models in adhering to human\\ninstructions (Zheng et al., 2023; Lu et al., 2023; Li\\net al., 2023). In these works, Vicuna’s evaluation\\nparadigm (Zheng et al., 2023) is widely adopted,']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put all together by creating a one final chain to read the user query, get the contexts from 5 different documents using the `retrieval_chain`, add both the question and context to the prompt, send it through the LLM, and get the final formatted output using  the `StrOutputParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "multi_query_chain = (\n",
    "    {'context': retrieval_chain, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When using LLMs to evaluate LLM generation, it is important to consider the following factors:\\n\\n1. **Reliability of LLMs as Evaluators**: LLMs can be sensitive to textual instructions and inputs, which may affect their evaluation consistency and reliability.\\n\\n2. **Complexity of Tasks**: Certain complex tasks, such as those requiring common-sense reasoning, may pose challenges for LLMs in accurately assessing performance.\\n\\n3. **Human Evaluation as a Benchmark**: While human evaluation is considered the most accurate measurement of model performance, it is costly and time-consuming. Therefore, the effectiveness of LLMs as evaluators should be compared against human evaluations.\\n\\n4. **Interpretability of Evaluation**: The evaluation pipeline should be simple and interpretable, allowing for clear understanding of how LLMs score and compare candidate responses.\\n\\n5. **Calibration and Robustness**: Calibration techniques may enhance the robustness of LLMs in evaluation tasks, helping to reduce conflicts in scoring.\\n\\n6. **Contextual Understanding**: LLMs need to effectively understand and adhere to human intent in their evaluations, which can vary based on the context of the generated content. \\n\\nThese considerations are crucial for ensuring that LLMs provide meaningful and accurate evaluations of generative models.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing all the above cells, you will be able to see a LangSmith trace like [this](https://smith.langchain.com/public/31d1e43a-3727-4d0b-82fb-2bbdf146dfac/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Fusion\n",
    "\n",
    "In the default multi-query approach, after we retrieved the relevant documents for each question generated for our original question, we take the union of all the documents to select only unique documents (same document can be retrieved by multiple questions). However, we did not pay attention to the rank of each document in the context, which is important for the LLM to produce the most correct answer. Because the each individual rank would help us to decide the top-k documents to select as the context if we have a huge number of documents with a limited context window of the LLM. Therefore in RAG Fusion, while we do exactly the same thing upto retrieving documents, we use [Reciprocal Rank Fusion (RRF)](https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking) to rank the each retrieved document before using them as the context to answer our original question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf(results: List[List], k=60):\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "            print(f\"Updating score for {doc} from {previous_score} to {fused_scores[doc]} based on rank {rank}\")\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(ranked_doc), score)\n",
    "        for ranked_doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the below code compared to the multi-query code we went through earlier is, now we use our `rrf` method instead of `get_context_union` to retrieve the final list of documents related to our original question (i.e., context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent assistant. Your task is to generate 4 questions based on the provided question in different wording and different perspectives to retrieve relevant documents from a vector database. By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "\n",
    "fusion_retrieval_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | generate_queries\n",
    "    | retriever.map()\n",
    "    | rrf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Document'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfusion_retrieval_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat need to consider when using LLM to eval LLM generation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4700\u001b[0m \n\u001b[1;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4716\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4723\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1926\u001b[0m         Output,\n\u001b[0;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1935\u001b[0m     )\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m, in \u001b[0;36mrrf\u001b[0;34m(results, k)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Update the score of the document using the RRF formula: 1 / (rank + k)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         fused_scores[doc_str] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (rank \u001b[38;5;241m+\u001b[39m k)\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating score for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfused_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m based on rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Sort the documents based on their fused scores in descending order to get the final reranked results\u001b[39;00m\n\u001b[1;32m     24\u001b[0m reranked_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     (loads(ranked_doc), score)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ranked_doc, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fused_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Document'"
     ]
    }
   ],
   "source": [
    "result = fusion_retrieval_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'P.; Tay, Y .; Shazeer, N. M.; Prabhakaran, V .; Reif, E.;\\nDu, N.; Hutchinson, B. C.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Pope, R.; Bradbury, J.;\\nAustin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.;\\nLevskaya, A.; Ghemawat, S.; Dev, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">S.; Michalewski,\\nH.; García, X.; Misra, V .; Robinson, K.; Fedus, L.;\\nZhou, D.; Ippolito, D.; Luan, D.; Lim, H.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zoph, B.;\\nSpiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.;\\nOmernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">M.;\\nLewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.;'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.05</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'the LLMs are already widely adopted as a proxy of\\nhuman evaluators, the reliability of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this paradigm'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04972677595628415</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Task-Oriented Dialogue(2308.08043).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;\\nMichalewski, H.; Garcia, X.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Misra, V .; Robinson, K.; Fe-\\ndus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;\\nSpiridonov, A.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Sepassi, R.; Dohan, D.; Agrawal, S.; Omer-\\nnick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\\nA.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;\\nWang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wei,\\nJ.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04918032786885246</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'13B in the Vicuna benchmark and demonstrate the\\neffectiveness of our proposed approach </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">through ex-\\nperimental results, which show closer alignment\\nwith human judgments.\\n2 Positional Bias of the LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Evaluator\\n2.1 LLMs as Evaluators\\nRecently, researchers have been utilizing LLMs\\nsuch as GPT-4 as evaluators to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compare the per-\\nformance of two AI assistants. As shown in Table\\n1, an evaluation template with three </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">placeholders\\nT(Q, R1, R2), is used to query the LLM for eval-'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.048915917503966164</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'with human intent (He et al., 2023). While hu-\\nman evaluation is treated as the most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accurate mea-\\nsurement of model performance, it is costly and\\ntime-consuming to operate at scales. Consider-\\ning</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the potent capabilities of LLMs, researchers\\nhave started utilizing LLMs to evaluate the profi-\\nciency of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative models in adhering to human\\ninstructions (Zheng et al., 2023; Lu et al., 2023; Li\\net al., 2023). In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these works, Vicuna’s evaluation\\nparadigm (Zheng et al., 2023) is widely adopted,'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04866871479774705</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Task-Oriented Dialogue(2308.08043).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Doctor, I have been experiencing some symptoms recently and I'm concerned that I might </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have contracted COVID-19. Could you please advise me on the necessary tests and precautions I should take?\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.04838709677419355</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'eration capabilities, becoming universal assistants\\n(OpenAI, 2022, 2023; Song et al., </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2023b). With\\nthe rapid advancement of LLMs, it becomes cru-\\ncial to evaluate their ability to follow human </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in-\\nstructions. Traditional evaluation methods assess\\nthe ability by calculating a metric, like BLEU,\\nROUGE, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">BERTScore, or BARTScore, to com-\\npare the generated response with a reference re-\\nsponse. However, these metrics </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">do not adequately\\nmeasure the alignment of the generated response'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.047891458495966696</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Scratch with Minimal Human Supervision. ArXiv,\\nabs/2305.03047.\\nTurpin, M.; Michael, J.;</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Perez, E.; and Bowman, S. R.\\n2023. Language Models Don’t Always Say What\\nThey Think: Unfaithful Explanations in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Chain-of-\\nThought Prompting. CoRR, abs/2305.04388.\\nWang, P.; Song, Y .; Liu, T.; Lin, B.; Cao, Y .; Li, S.;\\nand </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Sui, Z. 2022. Learning Robust Representations\\nfor Continual Relation Extraction via Adversarial\\nClass </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Augmentation. In Proceedings of the 2022'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.047619047619047616</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ample, The evaluation pipeline of Vicuna (Zheng\\net al., 2023) has gained significant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">candidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs (Dong </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  [cs.CL]  30 Aug 2023'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.047371031746031744</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Task-Oriented Dialogue(2308.08043).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"I haven't received a flu vaccine this year. I haven't experienced any shortness of breath</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">or difficulty breathing. My symptoms have remained mostly the same over the past week, with no significant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">improvement or worsening.\"</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.046875</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2020a,b), question answering (Min et al., 2019),\\nROC story cloze (Cai, Tu, and Gimpel, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2017;\\nSchwartz et al., 2017), lexical inference (Levy et al.,\\n2015), visual question answering (Goyal et </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al.,\\n2017), information extraction (Wang et al., 2021,\\n2022; Song et al., 2023a; Xia et al., 2023) and so </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on.\\nLLMs are pre-trained using a vast amount of data\\nfrom the internet, making it highly likely for them\\nto </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">learn biases present in those materials. Although'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016666666666666666</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Task-Oriented Dialogue(2308.08043).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'demonstrated remarkable performance on various natu-\\nral language processing (NLP) tasks</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">(Brown et al. 2020;\\nChowdhery et al. 2022; Wei et al. 2022a; OpenAI 2023).\\nLeveraging large-scale pre-training on</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">massive text corpora\\nand reinforcement learning from human feedback (RLHF),\\nLLMs not only possess a wide range of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge but\\nalso exhibit superior capabilities in language understand-\\ning, generation, interaction, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning. In many cases,'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016129032258064516</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'evaluators to analyze the characteristics of posi-\\ntional bias in LLM evaluators. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">find that:\\nLLMs are sensitive to the position of responses.\\nAs shown in Table 2, in the evaluation of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">“Vicuna-\\n13B v.s. ChatGPT” and “Vicuna-13B v.s. Alpaca-\\n13B”, when the order was changed, LLMs provide\\ndifferent</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluation results, e.g., the win rate of\\nVicuna-13B extremely differs when Vicuna-13B is\\nevaluated as Assistant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1 and Assistant 2.\\nTo empirically evaluate the sensitivity, we in-'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.016129032258064516</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Inference. In Proceedings of the 57th Annual Meet-\\ning of the Association for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Computational Linguistics\\n(ACL).\\nBowman, S. R. 2023. Eight things to know about large\\nlanguage models. arXiv </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preprint arXiv:2304.00612.\\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\\nplan, J.; Dhariwal, P.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Neelakantan, A.; Shyam, P.;\\nSastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\\nA.; Krueger, G.; Henighan, T.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Child, R.; Ramesh,\\nA.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.015625</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'and reduce conflict, showcasing how calibration\\nenhances LLM robustness.\\n5.4 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Fine-Grained Analysis of Evaluation\\nQuality\\nIn order to further analyze the evaluation capabili-\\nties of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model, we perform a fine-grained anal-\\nysis of the questions by dividing them into 9 cate-\\ngories following Zheng</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al. (2023). We calculate\\nthe performance of different evaluators within these\\ncategories. As shown in Figure </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6, we find that: 1)\\nIn certain complex tasks such as common-sense,'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.015625</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'P.; Tay, Y .; Shazeer, N. M.; Prabhakaran, V .; Reif, E.;\\nDu, N.; Hutchinson, B. C.; \u001b[0m\n",
       "\u001b[32mPope, R.; Bradbury, J.;\\nAustin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.;\\nLevskaya, A.; Ghemawat, S.; Dev, \u001b[0m\n",
       "\u001b[32mS.; Michalewski,\\nH.; García, X.; Misra, V .; Robinson, K.; Fedus, L.;\\nZhou, D.; Ippolito, D.; Luan, D.; Lim, H.; \u001b[0m\n",
       "\u001b[32mZoph, B.;\\nSpiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.;\\nOmernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, \u001b[0m\n",
       "\u001b[32mM.;\\nLewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.;'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.05\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'the LLMs are already widely adopted as a proxy of\\nhuman evaluators, the reliability of \u001b[0m\n",
       "\u001b[32mthis paradigm'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.04972677595628415\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for \u001b[0m\n",
       "\u001b[32mTask-Oriented Dialogue\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2308.08043\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;\\nMichalewski, H.; Garcia, X.; \u001b[0m\n",
       "\u001b[32mMisra, V .; Robinson, K.; Fe-\\ndus, L.; Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;\\nSpiridonov, A.; \u001b[0m\n",
       "\u001b[32mSepassi, R.; Dohan, D.; Agrawal, S.; Omer-\\nnick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\\nA.; \u001b[0m\n",
       "\u001b[32mMoreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;\\nWang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; \u001b[0m\n",
       "\u001b[32mWei,\\nJ.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.04918032786885246\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'13B in the Vicuna benchmark and demonstrate the\\neffectiveness of our proposed approach \u001b[0m\n",
       "\u001b[32mthrough ex-\\nperimental results, which show closer alignment\\nwith human judgments.\\n2 Positional Bias of the LLM \u001b[0m\n",
       "\u001b[32mEvaluator\\n2.1 LLMs as Evaluators\\nRecently, researchers have been utilizing LLMs\\nsuch as GPT-4 as evaluators to \u001b[0m\n",
       "\u001b[32mcompare the per-\\nformance of two AI assistants. As shown in Table\\n1, an evaluation template with three \u001b[0m\n",
       "\u001b[32mplaceholders\\nT\u001b[0m\u001b[32m(\u001b[0m\u001b[32mQ, R1, R2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, is used to query the LLM for eval-'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.048915917503966164\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'with human intent \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHe et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. While hu-\\nman evaluation is treated as the most \u001b[0m\n",
       "\u001b[32maccurate mea-\\nsurement of model performance, it is costly and\\ntime-consuming to operate at scales. Consider-\\ning\u001b[0m\n",
       "\u001b[32mthe potent capabilities of LLMs, researchers\\nhave started utilizing LLMs to evaluate the profi-\\nciency of \u001b[0m\n",
       "\u001b[32mgenerative models in adhering to human\\ninstructions \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng et al., 2023; Lu et al., 2023; Li\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In \u001b[0m\n",
       "\u001b[32mthese works, Vicuna’s evaluation\\nparadigm \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is widely adopted,'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.04866871479774705\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for \u001b[0m\n",
       "\u001b[32mTask-Oriented Dialogue\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2308.08043\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"Doctor\u001b[0m\u001b[32m, I have been experiencing some symptoms recently and I'm concerned that I might \u001b[0m\n",
       "\u001b[32mhave contracted COVID-19. Could you please advise me on the necessary tests and precautions I should take?\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.04838709677419355\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'eration capabilities, becoming universal assistants\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI, 2022, 2023; Song et al., \u001b[0m\n",
       "\u001b[32m2023b\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. With\\nthe rapid advancement of LLMs, it becomes cru-\\ncial to evaluate their ability to follow human \u001b[0m\n",
       "\u001b[32min-\\nstructions. Traditional evaluation methods assess\\nthe ability by calculating a metric, like BLEU,\\nROUGE, \u001b[0m\n",
       "\u001b[32mBERTScore, or BARTScore, to com-\\npare the generated response with a reference re-\\nsponse. However, these metrics \u001b[0m\n",
       "\u001b[32mdo not adequately\\nmeasure the alignment of the generated response'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.047891458495966696\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Scratch with Minimal Human Supervision. ArXiv,\\nabs/2305.03047.\\nTurpin, M.; Michael, J.;\u001b[0m\n",
       "\u001b[32mPerez, E.; and Bowman, S. R.\\n2023. Language Models Don’t Always Say What\\nThey Think: Unfaithful Explanations in \u001b[0m\n",
       "\u001b[32mChain-of-\\nThought Prompting. CoRR, abs/2305.04388.\\nWang, P.; Song, Y .; Liu, T.; Lin, B.; Cao, Y .; Li, S.;\\nand \u001b[0m\n",
       "\u001b[32mSui, Z. 2022. Learning Robust Representations\\nfor Continual Relation Extraction via Adversarial\\nClass \u001b[0m\n",
       "\u001b[32mAugmentation. In Proceedings of the 2022'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.047619047619047616\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'ample, The evaluation pipeline of Vicuna \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has gained significant \u001b[0m\n",
       "\u001b[32minterest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare \u001b[0m\n",
       "\u001b[32mcandidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear\u001b[0m\n",
       "\u001b[32mhow reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDong \u001b[0m\n",
       "\u001b[32met al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  30 Aug 2023'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.047371031746031744\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for \u001b[0m\n",
       "\u001b[32mTask-Oriented Dialogue\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2308.08043\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m\"I\u001b[0m\u001b[32m haven't received a flu vaccine this year. I haven't experienced any shortness of breath\u001b[0m\n",
       "\u001b[32mor difficulty breathing. My symptoms have remained mostly the same over the past week, with no significant \u001b[0m\n",
       "\u001b[32mimprovement or worsening.\"\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.046875\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'2020a,b\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, question answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMin et al., 2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nROC story cloze \u001b[0m\u001b[32m(\u001b[0m\u001b[32mCai, Tu, and Gimpel, \u001b[0m\n",
       "\u001b[32m2017;\\nSchwartz et al., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, lexical inference \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLevy et al.,\\n2015\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, visual question answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGoyal et \u001b[0m\n",
       "\u001b[32mal.,\\n2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, information extraction \u001b[0m\u001b[32m(\u001b[0m\u001b[32mWang et al., 2021,\\n2022; Song et al., 2023a; Xia et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and so \u001b[0m\n",
       "\u001b[32mon.\\nLLMs are pre-trained using a vast amount of data\\nfrom the internet, making it highly likely for them\\nto \u001b[0m\n",
       "\u001b[32mlearn biases present in those materials. Although'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.016666666666666666\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/DiagGPT - An LLM-based Chatbot with Automatic Topic Management for \u001b[0m\n",
       "\u001b[32mTask-Oriented Dialogue\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2308.08043\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'demonstrated remarkable performance on various natu-\\nral language processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tasks\u001b[0m\n",
       "\u001b[32m(\u001b[0m\u001b[32mBrown et al. 2020;\\nChowdhery et al. 2022; Wei et al. 2022a; OpenAI 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nLeveraging large-scale pre-training on\u001b[0m\n",
       "\u001b[32mmassive text corpora\\nand reinforcement learning from human feedback \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRLHF\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nLLMs not only possess a wide range of\u001b[0m\n",
       "\u001b[32mknowledge but\\nalso exhibit superior capabilities in language understand-\\ning, generation, interaction, and \u001b[0m\n",
       "\u001b[32mreasoning. In many cases,'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.016129032258064516\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'evaluators to analyze the characteristics of posi-\\ntional bias in LLM evaluators. We \u001b[0m\n",
       "\u001b[32mfind that:\\nLLMs are sensitive to the position of responses.\\nAs shown in Table 2, in the evaluation of \u001b[0m\n",
       "\u001b[32m“Vicuna-\\n13B v.s. ChatGPT” and “Vicuna-13B v.s. Alpaca-\\n13B”, when the order was changed, LLMs provide\\ndifferent\u001b[0m\n",
       "\u001b[32mevaluation results, e.g., the win rate of\\nVicuna-13B extremely differs when Vicuna-13B is\\nevaluated as Assistant \u001b[0m\n",
       "\u001b[32m1 and Assistant 2.\\nTo empirically evaluate the sensitivity, we in-'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.016129032258064516\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Inference. In Proceedings of the 57th Annual Meet-\\ning of the Association for \u001b[0m\n",
       "\u001b[32mComputational Linguistics\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mACL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nBowman, S. R. 2023. Eight things to know about large\\nlanguage models. arXiv \u001b[0m\n",
       "\u001b[32mpreprint arXiv:2304.00612.\\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\\nplan, J.; Dhariwal, P.; \u001b[0m\n",
       "\u001b[32mNeelakantan, A.; Shyam, P.;\\nSastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\\nA.; Krueger, G.; Henighan, T.; \u001b[0m\n",
       "\u001b[32mChild, R.; Ramesh,\\nA.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.015625\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1m(\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'and reduce conflict, showcasing how calibration\\nenhances LLM robustness.\\n5.4 \u001b[0m\n",
       "\u001b[32mFine-Grained Analysis of Evaluation\\nQuality\\nIn order to further analyze the evaluation capabili-\\nties of the \u001b[0m\n",
       "\u001b[32mmodel, we perform a fine-grained anal-\\nysis of the questions by dividing them into 9 cate-\\ngories following Zheng\u001b[0m\n",
       "\u001b[32met al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. We calculate\\nthe performance of different evaluators within these\\ncategories. As shown in Figure \u001b[0m\n",
       "\u001b[32m6, we find that: 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nIn certain complex tasks such as common-sense,'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;36m0.015625\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we format the context by considering only the page contents without meta data or re-ranking scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(documents: List):\n",
    "    return \"\\n\\n\".join([doc[0].page_content for doc in documents])\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Asnwer the given question using the provided context.\\n\\nContext: {context}\\n\\nQuestion: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "multi_query_chain = (\n",
    "    {'context': fusion_retrieval_chain | format_context, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Document'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmulti_query_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat need to consider when using LLM to eval LLM generation?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3727\u001b[0m, in \u001b[0;36mRunnableParallel.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3726\u001b[0m         ]\n\u001b[0;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: future\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3727\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m   3723\u001b[0m         futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3724\u001b[0m             executor\u001b[38;5;241m.\u001b[39msubmit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[1;32m   3725\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3726\u001b[0m         ]\n\u001b[0;32m-> 3727\u001b[0m         output \u001b[38;5;241m=\u001b[39m {key: \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[1;32m   3728\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3711\u001b[0m, in \u001b[0;36mRunnableParallel.invoke.<locals>._invoke_step\u001b[0;34m(step, input, config, key)\u001b[0m\n\u001b[1;32m   3709\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   3710\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m-> 3711\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4700\u001b[0m \n\u001b[1;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4716\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4723\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1926\u001b[0m         Output,\n\u001b[0;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1935\u001b[0m     )\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/miniconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m, in \u001b[0;36mrrf\u001b[0;34m(results, k)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# Update the score of the document using the RRF formula: 1 / (rank + k)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         fused_scores[doc_str] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (rank \u001b[38;5;241m+\u001b[39m k)\n\u001b[0;32m---> 21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating score for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfused_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m based on rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Sort the documents based on their fused scores in descending order to get the final reranked results\u001b[39;00m\n\u001b[1;32m     24\u001b[0m reranked_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     (loads(ranked_doc), score)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ranked_doc, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fused_scores\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Document'"
     ]
    }
   ],
   "source": [
    "multi_query_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing all the above cells, you will be able to see a LangSmith trace like [this](https://smith.langchain.com/public/99c5fb68-0ccf-4508-a72d-7c3a7b5e61d2/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Decomposition\n",
    "\n",
    "In \"Query Translation\", we focused on generating multiple questions from our original question with different perspectives (i.e., translate the query) to improve RAG.  \n",
    "However, the generated questions all do have the same meaning despite the wording is different, since it is in fact translation. Therefore, the answers for all the questions are somewhat similar. As a result, while the multi-query approach helps avoid ambiguities of the user query by writing it in different ways, `it will not help when the user query is complex` (e.g., a long mathematical computation).\n",
    "\n",
    "As a solution we can break down (i.e., decompose) the original query into multiple sub-problems (like in recursion or dynamic programming) and answer each sub-problem sequentially/parallelly to derive the answer to our original query. This simplifies the prompts and increases the context for the retrieval process. We do that using `\"Query Decomposition\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-to-Most Prompting\n",
    "\n",
    "First let's look at how to implement [Least-to-Most Prompting](https://arxiv.org/pdf/2205.10625) to break down a complex query into subquestions and answer them recursively to derive the final answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the multi-query and RAG fusion we first have generate a few questions based on our original questions. However our prompt should be different as we are generating sub questions by decomposing the original one, instead of generating the same question with different perspectives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "decompostion_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant that can break down complex questions into simpler parts. \\n\n",
    "        Your goal is to decompose the given question into multiple sub-questions that can be answerd in isolation to answer the main question in the end. \\n\n",
    "        Provide these sub-questions separated by one newline character. \\n\n",
    "        Original question: {question}\\n\n",
    "        Output (3 queries): \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "query_generation_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | decompostion_prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the key metrics to evaluate the quality of LLM-generated content?',\n",
       " 'What potential biases should be considered when using LLMs for evaluation?',\n",
       " 'How can human judgment be integrated into the evaluation of LLM outputs?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = query_generation_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")\n",
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the sub-questions, we iterate through them to answer them individually using the `least_to_most_chain`. We first extract the `question` from the user input using the `itemgetter` and provide it to our `retriever` to retrieve related documents as the `context`. `q_a_pairs` will also be provided as part of the user input. Then we populate our prompt and send to the LLM to get the answer. Each time we store the sub-question `Q_{n-1}` and its answer `A_{n-1}` since we provide them as the context to answer the question `Q_{n}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# Create the final prompt template to answer the question with provided context and background Q&A pairs\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "least_to_most_prompt = ChatPromptTemplate.from_template(template) \n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "least_to_most_chain = (\n",
    "        {'context': itemgetter('question') | retriever,\n",
    "        'q_a_pairs': itemgetter('q_a_pairs'),\n",
    "        'question': itemgetter('question'),\n",
    "        }\n",
    "        | least_to_most_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    answer = least_to_most_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pairs+=f\"Question: {q}\\n\\nAnswer: {answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting answers for the 3 generated sub-questions, finally we answer our original question by invoking the `least_to_most_chain` once again, but this time with the original question and all `q_a_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = least_to_most_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\", \"q_a_pairs\": q_a_pairs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">When using Large Language Models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> to evaluate the generation of other LLMs, several important considerations \n",
       "should be taken into account:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Reliability of LLMs as Evaluators**: LLMs may not always provide reliable evaluations due to their sensitivity\n",
       "to textual instructions and inputs. Variations in phrasing or context can lead to inconsistent assessments, which \n",
       "raises questions about their effectiveness as evaluators.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Positional Bias**: The order in which responses are presented can significantly influence evaluation outcomes.\n",
       "LLMs may exhibit positional bias, where the evaluation of a response can vary depending on whether it is assessed \n",
       "first or last. This can skew results and affect the perceived quality of the generated content.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Instruction Sensitivity**: LLMs can be highly sensitive to the specific wording and structure of prompts. This\n",
       "means that slight changes in how a task is framed can lead to different evaluation results, making it crucial to \n",
       "standardize evaluation prompts to ensure consistency.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Bias and Fairness**: LLMs may inadvertently perpetuate biases present in their training data, which can affect\n",
       "their evaluations. It is essential to analyze the outputs for potential biases and ensure that the evaluations do \n",
       "not reinforce stereotypes or unfair representations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Calibration of Evaluators**: The biases of human evaluators can influence how they interpret LLM evaluations. \n",
       "Calibrating human evaluators to recognize and minimize their biases is important for achieving fair assessments and\n",
       "aligning evaluations with objective quality metrics.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **Contextual Bias**: The context in which the evaluation occurs can introduce bias. LLMs trained on specific \n",
       "cultural or societal norms may evaluate content through a limited lens, potentially overlooking alternative \n",
       "perspectives. It is important to consider the diversity of contexts when interpreting evaluations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. **Combining Human Judgment with LLM Evaluations**: Integrating human judgment into the evaluation process can \n",
       "enhance the reliability and depth of assessments. Human evaluators can provide insights that LLMs may miss, \n",
       "particularly regarding nuances and contextual factors.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. **Iterative Evaluation Process**: Employing an iterative evaluation process where LLM outputs are continuously \n",
       "assessed and refined based on feedback can help identify recurring issues and improve the model's performance over \n",
       "time.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>. **Task-Specific Guidelines**: Developing specific evaluation criteria based on the type of content being \n",
       "assessed can enhance the relevance and accuracy of evaluations. Different tasks may require different metrics for \n",
       "quality assessment.\n",
       "\n",
       "By considering these factors, users can better navigate the complexities of using LLMs for evaluating LLM-generated\n",
       "content, leading to more reliable and fair assessments.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "When using Large Language Models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m to evaluate the generation of other LLMs, several important considerations \n",
       "should be taken into account:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Reliability of LLMs as Evaluators**: LLMs may not always provide reliable evaluations due to their sensitivity\n",
       "to textual instructions and inputs. Variations in phrasing or context can lead to inconsistent assessments, which \n",
       "raises questions about their effectiveness as evaluators.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Positional Bias**: The order in which responses are presented can significantly influence evaluation outcomes.\n",
       "LLMs may exhibit positional bias, where the evaluation of a response can vary depending on whether it is assessed \n",
       "first or last. This can skew results and affect the perceived quality of the generated content.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Instruction Sensitivity**: LLMs can be highly sensitive to the specific wording and structure of prompts. This\n",
       "means that slight changes in how a task is framed can lead to different evaluation results, making it crucial to \n",
       "standardize evaluation prompts to ensure consistency.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Bias and Fairness**: LLMs may inadvertently perpetuate biases present in their training data, which can affect\n",
       "their evaluations. It is essential to analyze the outputs for potential biases and ensure that the evaluations do \n",
       "not reinforce stereotypes or unfair representations.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Calibration of Evaluators**: The biases of human evaluators can influence how they interpret LLM evaluations. \n",
       "Calibrating human evaluators to recognize and minimize their biases is important for achieving fair assessments and\n",
       "aligning evaluations with objective quality metrics.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **Contextual Bias**: The context in which the evaluation occurs can introduce bias. LLMs trained on specific \n",
       "cultural or societal norms may evaluate content through a limited lens, potentially overlooking alternative \n",
       "perspectives. It is important to consider the diversity of contexts when interpreting evaluations.\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. **Combining Human Judgment with LLM Evaluations**: Integrating human judgment into the evaluation process can \n",
       "enhance the reliability and depth of assessments. Human evaluators can provide insights that LLMs may miss, \n",
       "particularly regarding nuances and contextual factors.\n",
       "\n",
       "\u001b[1;36m8\u001b[0m. **Iterative Evaluation Process**: Employing an iterative evaluation process where LLM outputs are continuously \n",
       "assessed and refined based on feedback can help identify recurring issues and improve the model's performance over \n",
       "time.\n",
       "\n",
       "\u001b[1;36m9\u001b[0m. **Task-Specific Guidelines**: Developing specific evaluation criteria based on the type of content being \n",
       "assessed can enhance the relevance and accuracy of evaluations. Different tasks may require different metrics for \n",
       "quality assessment.\n",
       "\n",
       "By considering these factors, users can better navigate the complexities of using LLMs for evaluating LLM-generated\n",
       "content, leading to more reliable and fair assessments.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the original question answer will look like [this](https://smith.langchain.com/public/7bd7f987-a53a-4d32-abb0-823940bc3f27/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead sequentially answering the sub-questions, we can use the LLM to answer them parallely and use those answers to derive the final answer to our main question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lc_hub_owner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'rlm'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lc_hub_repo'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'rag-prompt'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'lc_hub_commit_hash'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"You are an assistant for question-answering tasks. Use the following pieces of retrieved </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'lc_hub_owner'\u001b[0m: \u001b[32m'rlm'\u001b[0m,\n",
       "        \u001b[32m'lc_hub_repo'\u001b[0m: \u001b[32m'rag-prompt'\u001b[0m,\n",
       "        \u001b[32m'lc_hub_commit_hash'\u001b[0m: \u001b[32m'50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'context'\u001b[0m, \u001b[32m'question'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mtemplate\u001b[0m=\u001b[32m\"You\u001b[0m\u001b[32m are an assistant for question-answering tasks. Use the following pieces of retrieved \u001b[0m\n",
       "\u001b[32mcontext to answer the question. If you don't know the answer, just say that you don't know. Use three sentences \u001b[0m\n",
       "\u001b[32mmaximum and keep the answer concise.\\nQuestion: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mquestion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \\nContext: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontext\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \\nAnswer:\"\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "rich.print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_answer(question):\n",
    "    \n",
    "    questions = []\n",
    "    \n",
    "    sub_questions = query_generation_chain.invoke(question)\n",
    "    \n",
    "    sub_qa_chain = (\n",
    "        {'context': RunnablePassthrough() | retriever, 'question': RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    for q in sub_questions:\n",
    "        answer = sub_qa_chain.invoke(q)\n",
    "        questions.append({\"question\": q, \"answer\": answer})\n",
    "        \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_pairs = generate_and_answer(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What specific metrics or criteria should be used to evaluate LLM generation?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'To evaluate LLM generation, metrics such as adherence to human instructions and the ability to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">provide coherent and contextually relevant responses are essential. The Vicuna evaluation paradigm is noted for its</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">simplicity and interpretability, prompting models like GPT-4 to score and compare responses. Additionally, a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fine-grained analysis of evaluation quality can be performed by categorizing questions and assessing performance </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across these categories.'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are the potential biases or limitations of the LLM that may affect its generation?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The potential biases or limitations of LLMs include sensitivity to the position of responses, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which can lead to different evaluation results based on the order in which options are presented. Additionally, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">LLMs may be influenced by textual instructions and inputs, raising concerns about their reliability as evaluators. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">These factors can significantly affect the consistency and fairness of their generated outputs.'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'question'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'How can the context or input provided to the LLM influence the quality of its generation?'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The context or input provided to the LLM can significantly influence the quality of its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation, as LLMs are sensitive to textual instructions and inputs. This sensitivity means that variations in the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prompt can lead to different interpretations and outputs, affecting the overall performance. Consequently, the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">clarity and specificity of the input are crucial for achieving desired results.'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'question'\u001b[0m: \u001b[32m'What specific metrics or criteria should be used to evaluate LLM generation?'\u001b[0m,\n",
       "        \u001b[32m'answer'\u001b[0m: \u001b[32m'To evaluate LLM generation, metrics such as adherence to human instructions and the ability to \u001b[0m\n",
       "\u001b[32mprovide coherent and contextually relevant responses are essential. The Vicuna evaluation paradigm is noted for its\u001b[0m\n",
       "\u001b[32msimplicity and interpretability, prompting models like GPT-4 to score and compare responses. Additionally, a \u001b[0m\n",
       "\u001b[32mfine-grained analysis of evaluation quality can be performed by categorizing questions and assessing performance \u001b[0m\n",
       "\u001b[32macross these categories.'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'question'\u001b[0m: \u001b[32m'What are the potential biases or limitations of the LLM that may affect its generation?'\u001b[0m,\n",
       "        \u001b[32m'answer'\u001b[0m: \u001b[32m'The potential biases or limitations of LLMs include sensitivity to the position of responses, \u001b[0m\n",
       "\u001b[32mwhich can lead to different evaluation results based on the order in which options are presented. Additionally, \u001b[0m\n",
       "\u001b[32mLLMs may be influenced by textual instructions and inputs, raising concerns about their reliability as evaluators. \u001b[0m\n",
       "\u001b[32mThese factors can significantly affect the consistency and fairness of their generated outputs.'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'question'\u001b[0m: \u001b[32m'How can the context or input provided to the LLM influence the quality of its generation?'\u001b[0m,\n",
       "        \u001b[32m'answer'\u001b[0m: \u001b[32m'The context or input provided to the LLM can significantly influence the quality of its \u001b[0m\n",
       "\u001b[32mgeneration, as LLMs are sensitive to textual instructions and inputs. This sensitivity means that variations in the\u001b[0m\n",
       "\u001b[32mprompt can lead to different interpretations and outputs, affecting the overall performance. Consequently, the \u001b[0m\n",
       "\u001b[32mclarity and specificity of the input are crucial for achieving desired results.'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(qa_pairs):\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    \n",
    "    for i, qa in enumerate(qa_pairs):\n",
    "        formatted_string += f\"Question {i}: {qa['question']}\\nAnswer {i}: {qa['answer']}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(qa_pairs)\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Consider the following Question and Answer Pairs:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Use these to synthesize an answer to the question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final_rag_chain = (\n",
    "     prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = final_rag_chain.invoke({'context': context, 'question': \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">When using LLMs to evaluate LLM generation, several key factors need to be considered:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Evaluation Metrics**: It's important to establish specific metrics or criteria for evaluation, such as \n",
       "adherence to human instructions, coherence, and contextual relevance of responses. Utilizing frameworks like the \n",
       "Vicuna evaluation paradigm can enhance the interpretability and simplicity of the evaluation process.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Biases and Limitations**: Be aware of potential biases inherent in LLMs, such as sensitivity to the order of \n",
       "responses and the influence of textual instructions. These biases can lead to inconsistent evaluation results and \n",
       "may compromise the fairness and reliability of the outputs.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Context Sensitivity**: The quality of the LLM's generation is heavily influenced by the context or input \n",
       "provided. Variations in prompts can lead to different interpretations and outputs, so it is crucial to ensure that \n",
       "inputs are clear and specific to achieve the desired evaluation outcomes.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Fine-Grained Analysis**: Conducting a fine-grained analysis by categorizing questions and assessing \n",
       "performance across these categories can provide deeper insights into the evaluation quality and help identify \n",
       "specific areas for improvement.\n",
       "\n",
       "By considering these factors, one can enhance the effectiveness and reliability of using LLMs for evaluating LLM \n",
       "generation.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "When using LLMs to evaluate LLM generation, several key factors need to be considered:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Evaluation Metrics**: It's important to establish specific metrics or criteria for evaluation, such as \n",
       "adherence to human instructions, coherence, and contextual relevance of responses. Utilizing frameworks like the \n",
       "Vicuna evaluation paradigm can enhance the interpretability and simplicity of the evaluation process.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Biases and Limitations**: Be aware of potential biases inherent in LLMs, such as sensitivity to the order of \n",
       "responses and the influence of textual instructions. These biases can lead to inconsistent evaluation results and \n",
       "may compromise the fairness and reliability of the outputs.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Context Sensitivity**: The quality of the LLM's generation is heavily influenced by the context or input \n",
       "provided. Variations in prompts can lead to different interpretations and outputs, so it is crucial to ensure that \n",
       "inputs are clear and specific to achieve the desired evaluation outcomes.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Fine-Grained Analysis**: Conducting a fine-grained analysis by categorizing questions and assessing \n",
       "performance across these categories can provide deeper insights into the evaluation quality and help identify \n",
       "specific areas for improvement.\n",
       "\n",
       "By considering these factors, one can enhance the effectiveness and reliability of using LLMs for evaluating LLM \n",
       "generation.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for answering the original question will look like [this](https://smith.langchain.com/public/d5a17200-7752-42cb-87b9-146959e691bc/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step back prompting\n",
    "\n",
    "[Step back prompting](https://arxiv.org/pdf/2310.06117) allows LLMs to step back through in-context learning – prompting them to derive high-level abstractions such as concepts and principles for a specific example (i.e., Abstraction). Then, grounded on the documents regarding the high-level concept or principle, the LLM can reason about the solution to the original question (i.e., Reasoning).\n",
    "\n",
    "E.g., If the original question is \"What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?\", a possible step-back question would be \"What are the physics principles behind this question?\". Then the context (i.e., documents) retrieved for the step-back question will be used as additional context to answer the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate such step-back questions, we use few-shot learning to provide a few examples of (question, step-back question) pairs to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\\nHuman: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?\\nAI: What are the physics principles behind this question?\\nHuman: Estella Leopold went to which school between Aug 1954 and Nov 1954?\\nAI: What was Estella Leopold's education history?\\nHuman: What need to consider when using LLM to eval LLM generation?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                ('human', '{input}'), ('ai', '{output}')\n",
    "            ]\n",
    "        )\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "            # This is a prompt template used to format each individual example.\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                ('system', \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"),\n",
    "                few_shot_prompt,\n",
    "                ('user', '{question}'),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic\n",
       "step-back question, which is easier to answer. Here are a few examples:\n",
       "Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> and the \n",
       "volume is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span> and Nov <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span>?\n",
       "AI: What was Estella Leopold's education history?\n",
       "Human: What need to consider when using LLM to eval LLM generation?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic\n",
       "step-back question, which is easier to answer. Here are a few examples:\n",
       "Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of \u001b[1;36m2\u001b[0m and the \n",
       "volume is increased by a factor of \u001b[1;36m8\u001b[0m?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug \u001b[1;36m1954\u001b[0m and Nov \u001b[1;36m1954\u001b[0m?\n",
       "AI: What was Estella Leopold's education history?\n",
       "Human: What need to consider when using LLM to eval LLM generation?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(final_prompt.format(question= \"What need to consider when using LLM to eval LLM generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the created few-shot prompt to generate the step-back question through a chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What factors should be taken into account when evaluating the output of language models?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_query_chain = (\n",
    "    {'question': RunnablePassthrough()}\n",
    "    | final_prompt \n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0.7) \n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "step_back_query_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What factors should be taken into account when evaluating the output of language models?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_query_chain = (\n",
    "    final_prompt \n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0.9) \n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "step_back_query_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use both the context retrieved for the original question and the context retrieved for the step-back question to answer our original question via the `step_back_chain`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_chain = (\n",
    "    {'normal_context': RunnablePassthrough() | retriever,\n",
    "     'step_back_context': RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "     'question': RunnablePassthrough()\n",
    "     }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When using large language models (LLMs) to evaluate the generation of other LLMs, several important considerations should be taken into account:\\n\\n1. **Reliability of LLMs as Evaluators**: LLMs are increasingly being used as proxies for human evaluators due to their efficiency and scalability. However, their reliability in this role is still under scrutiny. Research indicates that LLMs can be sensitive to the textual instructions and inputs they receive, which may lead to inconsistent evaluations (Dong et al., 2022; Turpin et al., 2023). This sensitivity can affect the fairness and accuracy of the evaluation process.\\n\\n2. **Evaluation Paradigms**: The evaluation pipeline, such as that used by Vicuna, prompts LLMs like GPT-4 to score and compare candidate responses while providing explanations. While this method is valued for its simplicity and interpretability, it is essential to critically assess how well these evaluations align with human judgment and the specific criteria being measured.\\n\\n3. **Human Intent and Context**: Evaluating LLM outputs requires an understanding of human intent and context. LLMs may not fully grasp nuanced human instructions or the subtleties of context, which can lead to misinterpretations in their evaluations (He et al., 2023). Ensuring that the evaluation criteria are clear and that the LLMs are capable of understanding them is crucial.\\n\\n4. **Cost and Time Efficiency**: While human evaluation is often considered the gold standard for assessing model performance, it is also costly and time-consuming, especially at scale. LLMs can provide a more efficient alternative, but this efficiency must be balanced against the potential for bias and inaccuracies in their evaluations.\\n\\n5. **Comparative Analysis**: When using LLMs for evaluation, it is beneficial to compare their assessments with those of human evaluators to identify discrepancies and understand the limitations of LLM-based evaluations. This comparative analysis can help refine the evaluation process and improve the reliability of LLMs as evaluators.\\n\\n6. **Continuous Improvement**: As LLMs evolve, so too should the evaluation methodologies. Ongoing research and development are necessary to enhance the capabilities of LLMs in evaluation tasks, ensuring they can adapt to new types of content and evaluation criteria.\\n\\nIn summary, while LLMs offer a promising avenue for evaluating generative models, careful consideration of their reliability, sensitivity to instructions, understanding of human intent, and the need for comparative analysis with human evaluations is essential for effective and fair assessment.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'You are an expert of world knowledge. \\nI am going to ask you a question. Your response should</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be comprehensive and not contradicted with the following context if they are relevant. \\nOtherwise, ignore them if </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">they are not relevant.\\n\\n&lt;normal_context&gt;\\n# What need to consider when using LLM to eval LLM </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generation?\\n&lt;/normal_context&gt;\\n\\n&lt;step_back_context&gt;\\n# What factors should be taken into account when assessing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the output of language models?\\n&lt;/step_back_context&gt;\\n\\n\\n# Original Question: What need to consider when using LLM</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to eval LLM generation?\\n# Answer:'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response_metadata</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mHumanMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[32m'You are an expert of world knowledge. \\nI am going to ask you a question. Your response should\u001b[0m\n",
       "\u001b[32mbe comprehensive and not contradicted with the following context if they are relevant. \\nOtherwise, ignore them if \u001b[0m\n",
       "\u001b[32mthey are not relevant.\\n\\n\u001b[0m\u001b[32m<\u001b[0m\u001b[32mnormal_context\u001b[0m\u001b[32m>\\n# What need to consider when using LLM to eval LLM \u001b[0m\n",
       "\u001b[32mgeneration?\\n</normal_context>\\n\\n<step_back_context>\\n# What factors should be taken into account when assessing \u001b[0m\n",
       "\u001b[32mthe output of language models?\\n</step_back_context\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\\n\\n\\n# Original Question: What need to consider when using LLM\u001b[0m\n",
       "\u001b[32mto eval LLM generation?\\n# Answer:'\u001b[0m,\n",
       "            \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mresponse_metadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_step_back_chain = (\n",
    "    {'normal_context': RunnablePassthrough(),\n",
    "     'step_back_context': RunnablePassthrough() | step_back_query_chain,\n",
    "     'question': RunnablePassthrough()\n",
    "     }\n",
    "    | response_prompt)\n",
    "\n",
    "res = test_step_back_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")\n",
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LangSmith trace for the implemented step-back prompting chain will look like [this](https://smith.langchain.com/public/425c098b-47ae-4f53-9259-8cd6b567a2b0/r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we looked at ways to improve the LLMs answers to a user query through the \"Query Transformation\". In summary, query transformation may help us to remove ambiguities of the user query and simplify it through techniques such as ,\n",
    "\n",
    "- **Multi-query**: That re-writes the question in different perspectives (i.e., sub-questions).\n",
    "- **RAG Fusion**: That not only re-writes the question in different perspectives, but also rank the documents retrieved for each sub-question to provide the most relevant information to answer the original question.\n",
    "\n",
    "- **Least-to-Most Prompting**: That helps break-down complex questions into mutiple sub problems and answer the final question using the sub problems and their answers as the context.\n",
    "- **Step-back Prompting**: That generates a step-back question and use the retrieved documents for that step-back question as the additional context to answer the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we will  generate Hypothetical Documents, instead of questions to help LLMs answer questions more accurately through [HyDE](https://arxiv.org/pdf/2212.10496) (Hypothetical Document Embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
