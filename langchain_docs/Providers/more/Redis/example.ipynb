{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Redis](https://python.langchain.com/v0.2/docs/integrations/providers/redis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache\n",
    "The Cache wrapper allows for Redis to be used as a remote, low-latency, in-memory cache for LLM prompts and responses.\n",
    "\n",
    "Standard Cache\n",
    "The standard cache is the Redis bread & butter of use case in production for both open-source and enterprise users globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import RedisCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_url = \"redis://:@localhost:6379/0\" # I do\"n't set username and password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "import redis\n",
    "\n",
    "redis_client = redis.Redis.from_url(redis_url)\n",
    "set_llm_cache(RedisCache(redis_client))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 4.93 ms, total: 19.1 ms\n",
      "Wall time: 1.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's one for you:\\n\\nWhy don't skeletons fight each other?\\n\\nThey don't have the guts!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 11, 'total_tokens': 32}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-53eb429d-e9c4-418f-b965-3994f75a3165-0', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 ms, sys: 608 μs, total: 2.68 ms\n",
      "Wall time: 2.36 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's one for you:\\n\\nWhy don't skeletons fight each other?\\n\\nThey don't have the guts!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 11, 'total_tokens': 32}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-53eb429d-e9c4-418f-b965-3994f75a3165-0', usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 ms, sys: 4.67 ms, total: 18.9 ms\n",
      "Wall time: 3.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. Here's a simplified explanation:\\n\\n1. **Scattering of Sunlight**: Sunlight, or white light, is composed of various colors, each with different wavelengths. When sunlight enters Earth's atmosphere, it collides with gas molecules and small particles.\\n\\n2. **Wavelength Dependence**: Shorter wavelengths of light (blue and violet) are scattered in all directions more efficiently than longer wavelengths (red and yellow). This is because shorter wavelengths interact more with the small particles and gas molecules in the atmosphere.\\n\\n3. **Human Perception**: Even though violet light is scattered even more than blue light, our eyes are more sensitive to blue light and less so to violet. Additionally, some of the violet light is absorbed by the upper atmosphere.\\n\\n4. **Resulting Color**: Because blue light is scattered in all directions and our eyes are more sensitive to it, we perceive the sky as blue when we look in directions away from the sun.\\n\\nIn summary, the blue appearance of the sky is due to the scattering of sunlight by the molecules and particles in the Earth's atmosphere, with blue light being scattered more than other colors because of its shorter wavelength.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 14, 'total_tokens': 259}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-11d9725b-f2c1-4c94-b48c-62da68b3cbd6-0', usage_metadata={'input_tokens': 14, 'output_tokens': 245, 'total_tokens': 259})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(\"Tell me why sky is blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.29 ms, sys: 1.43 ms, total: 4.72 ms\n",
      "Wall time: 4.68 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The sky appears blue primarily due to a phenomenon called Rayleigh scattering. Here's a simplified explanation:\\n\\n1. **Scattering of Sunlight**: Sunlight, or white light, is composed of various colors, each with different wavelengths. When sunlight enters Earth's atmosphere, it collides with gas molecules and small particles.\\n\\n2. **Wavelength Dependence**: Shorter wavelengths of light (blue and violet) are scattered in all directions more efficiently than longer wavelengths (red and yellow). This is because shorter wavelengths interact more with the small particles and gas molecules in the atmosphere.\\n\\n3. **Human Perception**: Even though violet light is scattered even more than blue light, our eyes are more sensitive to blue light and less so to violet. Additionally, some of the violet light is absorbed by the upper atmosphere.\\n\\n4. **Resulting Color**: Because blue light is scattered in all directions and our eyes are more sensitive to it, we perceive the sky as blue when we look in directions away from the sun.\\n\\nIn summary, the blue appearance of the sky is due to the scattering of sunlight by the molecules and particles in the Earth's atmosphere, with blue light being scattered more than other colors because of its shorter wavelength.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 245, 'prompt_tokens': 14, 'total_tokens': 259}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-11d9725b-f2c1-4c94-b48c-62da68b3cbd6-0', usage_metadata={'input_tokens': 14, 'output_tokens': 245, 'total_tokens': 259})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(\"Tell me why sky is blue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Cache\n",
    "Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import RedisSemanticCache\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.globals import set_llm_cache\n",
    "import redis\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    # With the `text-embedding-3` class\n",
    "    # of models, you can specify the size\n",
    "    # of the embeddings you want returned.\n",
    "    # dimensions=1024\n",
    ")\n",
    "# must use DB0\n",
    "redis_url = \"redis://:@localhost:6379/0\" # I do\"n't set username and password\n",
    "\n",
    "set_llm_cache(RedisSemanticCache(\n",
    "    embedding=embeddings,\n",
    "    redis_url=redis_url\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 217 ms, sys: 46.4 ms, total: 263 ms\n",
      "Wall time: 814 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 11, 'total_tokens': 30}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c1128af-ee20-4e5f-93df-68ebfb449d03-0', usage_metadata={'input_tokens': 11, 'output_tokens': 19, 'total_tokens': 30})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 第一次调用时，尚未缓存，所以需要更长的时间\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 ms, sys: 4.44 ms, total: 15.1 ms\n",
      "Wall time: 548 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure, here's one for you:\\n\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 11, 'total_tokens': 30}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-2c1128af-ee20-4e5f-93df-68ebfb449d03-0', usage_metadata={'input_tokens': 11, 'output_tokens': 19, 'total_tokens': 30})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 第二次调用时，虽然不是直接命中，但问题在语义上与原始问题相似，\n",
    "# 因此使用了缓存的结果！\n",
    "llm.invoke(\"Tell me one joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
