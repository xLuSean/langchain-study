{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: [LangChain - Custom Output Parsers](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/custom/)\n",
    "\n",
    "# Custom Output Parsers\n",
    "In some situations you may want to implement a custom parser to structure the model output into a custom format.\n",
    "\n",
    "There are two ways to implement a custom parser:\n",
    "\n",
    "1. Using `RunnableLambda` or `RunnableGenerator` in LCEL -- we strongly recommend this for most use cases\n",
    "2. By inherting from one of the base classes for out parsing -- this is the hard way of doing things\n",
    "\n",
    "The difference between the two approaches are mostly superficial and are mainly in terms of which callbacks are triggered (e.g., `on_chain_start` vs. `on_parser_start`), and how a runnable lambda vs. a parser might be visualized in a tracing platform like LangSmith.\n",
    "\n",
    "## Runnable Lambdas and Generators\n",
    "The recommended way to parse is using runnable lambdas and runnable generators!\n",
    "\n",
    "Here, we will make a simple parse that inverts the case of the output from the model.\n",
    "\n",
    "For example, if the model outputs: \"Meow\", the parser will produce \"mEOW\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "# from langchain_anthropic.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# model = ChatAnthropic(model_name=\"claude-2.1\")\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "# model = ChatOpenAI(model_name=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def parse(ai_message: AIMessage) -> str:\n",
    "    \"\"\"Parse the AI message.\"\"\"\n",
    "    return ai_message.content.swapcase()\n",
    "\n",
    "\n",
    "chain = model | parse\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tip\n",
    "LCEL automatically upgrades the function parse to RunnableLambda(parse) when composed using a | syntax.\n",
    "\n",
    "If you don't like that you can manually import RunnableLambda and then runparse = RunnableLambda(parse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does streaming work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(\"Hello\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, it doesn't because the parser aggregates the input before parsing the output.\n",
    "\n",
    "If we want to implement a streaming parser, we can have the parser accept an iterable over the input instead and yield the results as they're available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableGenerator\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "def streaming_parse(chunks: Iterable[AIMessageChunk]) -> Iterable[str]:\n",
    "    for chunk in chunks:\n",
    "        yield chunk.content.swapcase()\n",
    "\n",
    "streaming_parse = RunnableGenerator(streaming_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tip\n",
    "Please wrap the streaming parser in RunnableGenerator as we may stop automatically upgrading it with the | syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | streaming_parse\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain.stream(\"tell me about yourself in one sentence\"):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inherting from Parsing Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
