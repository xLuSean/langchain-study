{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LangChain 怎麼玩？ LCEL (LangChain Expression Language) 篇，一定要認識的 LangChain 核心](https://myapollo.com.tw/blog/langchain-expression-language/#google_vignette)\n",
    "\n",
    "## LCEL 有 2 個重要的目的。\n",
    "\n",
    "### 統一的協定(Protocol) \n",
    "1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, …). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object.\n",
    "\n",
    "LangChain 制定的協定(protocol)，每個元件例如(Prompt, ChatModel, LLM, OutputParser, Retriever, Tool 這些都是 LangChain 基本元件)都必須實作 1 個稱為 “Runnable” 的協定，這個協定至少包含以下方法（這些方法很重要，如果要將 Chain 應用變成 API 開放給他人使用，也可能需要實作以下方法）：\n",
    "\n",
    "* invoke (支援單一輸入、單一輸出)\n",
    "* batch (支援多個輸入、多個輸出)\n",
    "* stream (支援有部分結果就輸出的模式)\n",
    "* ainvoke (async 版本的 invoke)\n",
    "* abatch (async 版本的 batch)\n",
    "* astream (async 版本的 stream)\n",
    "\n",
    "### 提供組合原型(Composition primitives)，讓 Chain 的開發變得簡易 \n",
    "2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.\n",
    "\n",
    "LCEL 提供元件組合的功能，也就是本系列文章中經常使用到的 | 運算子，例如：\n",
    "```\n",
    "prompt | llm\n",
    "```\n",
    "組合出來的 Chain 原型(primitives)分為 2 種：\n",
    "\n",
    "* RunnableSequence\n",
    "* RunnableParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableSequence #\n",
    "RunnableSequence 會依序執行每個 Runnable , 例如 prompt | llm | output 裡的 prompt, llm, output 都是 Runnable , 它們透過 | 運算子串起來之後，就是 RunnableSequence ，執行上是依序執行，例如下列程式碼範例，可以體驗到何謂 RunnableSequence :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "prompt_output = prompt.invoke({\"input\": 'Hi there'})\n",
    "llm_output = llm.invoke(prompt_output)\n",
    "answer = StrOutputParser().invoke(llm_output)\n",
    "\n",
    "print(llm_output)\n",
    "print(\"=======================\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "res = chain.invoke({\"input\": 'Hi there'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel #\n",
    "RunnableParallel 則是可以同時執行 Runnable, 它的範例可以用 LangChain 的 RunnableLambda 體驗："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x: int) -> int:\n",
    "    return x + 2\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(add_two)\n",
    "\n",
    "parallel = {\"runnable_1\": runnable_1, \"runnable_2\": runnable_2}\n",
    "\n",
    "chain = RunnableLambda(lambda x: x) | parallel\n",
    "answer = chain.invoke(1)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = {\"r1\": runnable_1, \"r2\": runnable_2}\n",
    "\n",
    "chain = RunnableLambda(lambda x: x) | parallel\n",
    "answer = chain.invoke(1)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch \n",
    "目前為止，我們已經接觸 `Runnable`, `RunnableSequence`, `RunnableParallel`, `RunnableLambda`, 還有個變化型稱為 `RunnableBranch`。\n",
    "\n",
    "`RunnableBranch` 可以讓我們動態決定要走哪個分支執行下 1 個 Runnable, 例如你可以透過使用者不同的輸入，而採用不同的語言模型，例如：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "python_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a full-stack developer\"),\n",
    "    (\"user\", \"python {input}\"),\n",
    "])\n",
    "\n",
    "default_chain = default_prompt | llm # you can customize the chain here\n",
    "python_chain = python_prompt | llm  # you can customize the chain here\n",
    "\n",
    "def route(x):\n",
    "    if 'python' in x['input']:\n",
    "        return python_chain\n",
    "    return default_chain\n",
    "\n",
    "chain = RunnableLambda(route)\n",
    "\n",
    "print(type(chain))\n",
    "\n",
    "# print(chain.invoke({\"input\": \"python is the best\"}))\n",
    "\n",
    "llm_output = chain.invoke({\"input\": \"python is the best\"})\n",
    "\n",
    "res = StrOutputParser().invoke(llm_output)\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from LangChain\n",
    "ref: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.branch.RunnableBranch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: isinstance(x, str), lambda x: x.upper()),\n",
    "    (lambda x: isinstance(x, int), lambda x: x + 1),\n",
    "    (lambda x: isinstance(x, float), lambda x: x * 2),\n",
    "    lambda x: \"goodbye\",\n",
    ")\n",
    "\n",
    "print(branch.invoke(\"hello\")) # \"HELLO\"\n",
    "print(branch.invoke(None)) # \"goodbye\"\n",
    "print(branch.invoke(1)) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough \n",
    "```\n",
    "Runnable to passthrough inputs unchanged or with additional keys.\n",
    "```\n",
    "如果你想對輸入做些加工的話，或者對前者 Runnable 輸出的結果做修改的話，例如修改使用者 prompt 加入關鍵詞，則可以使用 RunnablePassthrough 。\n",
    "\n",
    "下列是修改使用者 prompt 的範例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = RunnablePassthrough.assign(input=lambda x: x['input'] + ' this is important to me.') | prompt\n",
    "print(chain.invoke({\"input\": \"python is the best.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入/輸出 (Input & Output Schema) \n",
    "從各個範例可以得知每個 Runnable 都有輸入與輸出，可是要怎麼知道它到底需要輸入什麼結構的資料，以及它會輸出什麼結構的資料呢？\n",
    "\n",
    "這時可以查看 Runnable 的輸入與輸出的 schema, 這些 schema 被存在 `input_schema` 與 `output_schema` 兩個屬性中，只要是 Runnable 都可以查看其輸入與輸出結構。\n",
    "\n",
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.input_schema)\n",
    "print(prompt.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.output_schema)\n",
    "print(prompt.output_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(\"you are a full-stack developer\"),\n",
    "    # MessagesPlaceholder(variable_name='chat_history'),\n",
    "    HumanMessage(\"python {input1}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt.input_schema)\n",
    "print(prompt.input_schema.schema())\n",
    "print(prompt.output_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = Ollama(model='llama2')\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.input_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列印 Chain 的樣子 \n",
    "除了 input / output schema 的資訊， LangChain 也有提供 1 個方便的函式可以查看 chain 的長相，該方法為 <chain>.get_graph().print_ascii() 。\n",
    "\n",
    "舉下列範例為例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
