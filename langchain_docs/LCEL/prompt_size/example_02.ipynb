{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref: MyAppollo [LangChain 怎麼玩？為了荷包著想，管好你的 prompt 長度(size)](https://myapollo.com.tw/blog/langchain-managing-prompt-size/#google_vignette)\n",
    "\n",
    "## 這個例子相較於LangChain用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "number_of_tokens = llm.get_num_tokens_from_messages([HumanMessage(\"Hi, there. How are you today?\")])\n",
    "print(f\"Tokens: {number_of_tokens}\")\n",
    "\n",
    "num = llm.get_num_tokens('Hi, there. How are you today?')\n",
    "print(f'Tokens: {num}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在把prompt傳給LLM之前，修改prompt的長度\n",
    "```\n",
    "prompt | modify promtp | LLM\n",
    "```\n",
    "prompt template 在加入 input value 之後會變成`ChatPromptValue`的實例(instance)，該實例可以呼叫`.to_messages()`，再丟回`.get_num_tokens_from_messages`得到tokens的總數。超過預計長度時，我們可以把舊的messages丟掉，再把剩下的messages用ChatPromptValue回傳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompt_values import ChatPromptValue\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# >>> original code, incorrect >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:\n",
    "#     messages = prompt.to_messages()\n",
    "#     print(f\"===========================\\n {messages}\\n===========================\")\n",
    "#     num_tokens = llm.get_num_tokens_from_messages(messages)\n",
    "#     recent_messages = messages[2:]\n",
    "#     while num_tokens > 50:\n",
    "#         recent_messages = recent_messages[2:] # remove 2 messages。每一輪對話會有一次Human，一次AI\n",
    "#         num_tokens = llm.get_num_tokens_from_messages(\n",
    "#             messages[:2] + recent_messages\n",
    "#         )\n",
    "#     # also update chat history\n",
    "#     chat_history = recent_messages\n",
    "#     messages = messages[:2] + recent_messages\n",
    "#     return ChatPromptValue(messages=messages)\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# >>> Sean modified code >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def condense_prompt(prompt: ChatPromptValue) -> ChatPromptValue:\n",
    "    messages = prompt.to_messages()\n",
    "    # print(f\"===========================\\n {messages}\\n===========================\")\n",
    "    num_tokens = llm.get_num_tokens_from_messages(messages)\n",
    "    recent_messages = messages[1:]\n",
    "    # while ( num_tokens > 100 and len(recent_messages) > 4 ):\n",
    "    while num_tokens > 50:\n",
    "        # remove 2 messages。每一輪對話會有一次Human，一次AI. 但是我們開頭有system message\n",
    "        sys_message = messages[0]\n",
    "        recent_messages = messages[3:]\n",
    "        messages = [sys_message] + recent_messages\n",
    "        # print(f\"===========================\\n {messages}\\n===========================\")\n",
    "        num_tokens = llm.get_num_tokens_from_messages(\n",
    "            [sys_message] + recent_messages\n",
    "        )\n",
    "    # also update chat history\n",
    "    chat_history = recent_messages\n",
    "    messages = [messages[0]] + recent_messages\n",
    "    return ChatPromptValue(messages=messages)\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a powerful chat bot.'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | condense_prompt | llm\n",
    "\n",
    "input_text = input('>>> ')\n",
    "while input_text.lower() != 'bye':\n",
    "    if input_text:\n",
    "        print(f\"===========================\\n {chat_history}\\n===========================\")\n",
    "        response = chain.invoke({\n",
    "            'input': input_text,\n",
    "            'chat_history': chat_history,\n",
    "        })\n",
    "        chat_history.append(HumanMessage(content=input_text))\n",
    "        chat_history.append(AIMessage(content=response.content))\n",
    "        # print(f\"===========================\\n {chat_history}\\n===========================\")\n",
    "        print(response.content)\n",
    "    input_text = input('>>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
