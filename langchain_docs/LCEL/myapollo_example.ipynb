{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://myapollo.com.tw/blog/langchain-expression-language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableSequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot: Greetings, human. I'm happy to assist you with any questions or topics you'd like to discuss. What's on your mind today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = Ollama(model='llama3')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "prompt_output = prompt.invoke({\"input\": 'Hi there'})\n",
    "llm_output = llm.invoke(prompt_output)\n",
    "answer = StrOutputParser().invoke(llm_output)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runnable_1': 2, 'runnable_2': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x: int) -> int:\n",
    "    return x + 2\n",
    "\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(add_two)\n",
    "\n",
    "parallel = {\"runnable_1\": runnable_1, \"runnable_2\": runnable_2}\n",
    "\n",
    "chain = RunnableLambda(lambda x: x) | parallel\n",
    "answer = chain.invoke(1)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r1': 2, 'r2': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x: int) -> int:\n",
    "    return x + 2\n",
    "\n",
    "\n",
    "runnable_1 = RunnableLambda(add_one)\n",
    "runnable_2 = RunnableLambda(add_two)\n",
    "\n",
    "parallel = RunnableParallel(r1=runnable_1, r2=runnable_2)\n",
    "\n",
    "chain = RunnableLambda(lambda x: x) | parallel\n",
    "answer = chain.invoke(1)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, so I don't have a personal name. But if you want to give me one, that's okay too! Some people call me \"Assistant\" or \"AI Assistant\", while others like to give me more playful names like \"Chatbot\" or \"Language Model\". You can choose whatever name you like for me - just keep in mind that I'll always be here to help and assist you with your questions and tasks!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = Ollama(model='llama3')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "default_chain = prompt | llm # you can customize the chain here\n",
    "python_chain = prompt | llm  # you can customize the chain here\n",
    "\n",
    "def route(x):\n",
    "    if 'python' in x['input']:\n",
    "        return python_chain\n",
    "    return default_chain\n",
    "\n",
    "chain = RunnableLambda(route)\n",
    "# print(chain.invoke({\"input\": \"python is the best\"}))\n",
    "print(chain.invoke({\"input\": \"what's you name?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was created by a team of software developers using a range of technologies, including natural language processing (NLP) and machine learning. My creators are the researchers at Meta AI, a company that focuses on developing artificial intelligence models for various applications.\n",
      "\n",
      "More specifically, I am based on a type of AI model called a transformer, which is designed to process and generate human-like text. This architecture was introduced in a research paper by Vaswani et al. in 2017 and has since been widely adopted in the NLP community.\n",
      "\n",
      "My training data consists of a massive corpus of text, which I use to learn patterns and relationships in language. This corpus is sourced from various places, including books, articles, and websites, and is used to train me on how to generate human-like responses to user input.\n",
      "\n",
      "So, while I don't have a single \"creator\" in the classical sense, I am the result of the collective efforts of many researchers and developers who have contributed to the development of AI technologies over the years.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm_llama3 = Ollama(model='llama3')\n",
    "llm_mistral = Ollama(model='mistral')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "default_chain = prompt | llm_llama3\n",
    "python_chain = prompt | llm_mistral\n",
    "\n",
    "def route(x):\n",
    "    if 'python' in x['input']:\n",
    "        return python_chain\n",
    "    return default_chain\n",
    "\n",
    "chain = RunnableLambda(route)\n",
    "# print(chain.invoke({\"input\": \"python is the best\"}))\n",
    "print(chain.invoke({\"input\": \"who create you?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='python is the best. this is important to me.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = RunnablePassthrough.assign(input=lambda x: x['input'] + ' this is important to me.') | prompt\n",
    "print(chain.invoke({\"input\": \"python is the best.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸入/輸出 (Input & Output Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'PromptInput', 'type': 'object', 'properties': {'input': {'title': 'Input', 'type': 'string'}}}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.input_schema.schema())\n",
    "# print(prompt.output_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'PromptInput', 'type': 'object', 'properties': {'input': {'title': 'Input', 'type': 'string'}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = Ollama(model='llama3')\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.input_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 列印 Chain 的樣子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+    \n",
      "    | PromptInput |    \n",
      "    +-------------+    \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "+--------------------+ \n",
      "| ChatPromptTemplate | \n",
      "+--------------------+ \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "      +--------+       \n",
      "      | Ollama |       \n",
      "      +--------+       \n",
      "           *           \n",
      "           *           \n",
      "           *           \n",
      "   +--------------+    \n",
      "   | OllamaOutput |    \n",
      "   +--------------+    \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = Ollama(model='llama3')\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
