{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Greg Kamradt's semantic chunking for Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../text_files/your_cat_part_3.txt') as file:\n",
    "    essay = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-pattern split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'(?<=[。？；!])\\s*|(?<=\\.)\\s*\\n'\n",
    "single_sentences_list = re.split(pattern, essay)\n",
    "print(f\"{len(single_sentences_list)} senteneces were found\")\n",
    "print(f\"max length of sentence: {max(len(sentence) for sentence in single_sentences_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-patterns split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern1 = r'={16}'\n",
    "pattern2 = r'(?<=[。？；!])\\s*|(?<=\\.)\\s*\\n'\n",
    "patterns = [pattern1, pattern2]\n",
    "\n",
    "def split_by_patterns(text, patterns):\n",
    "    result = [text]\n",
    "    for pattern in patterns:\n",
    "        temp = []\n",
    "        for segment in result:\n",
    "            temp.extend(re.split(pattern, segment))\n",
    "        result = temp\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentences_list = split_by_patterns(essay, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(single_sentences_list)} senteneces were found\")\n",
    "print(f\"max length of sentence: {max(len(sentence) for sentence in single_sentences_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(single_sentences_list[0:5])\n",
    "for i, sentence in enumerate(single_sentences_list[:5]):\n",
    "    print(f\"sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for combine sentences into group\n",
    "ex: group1: 1,2,3; group2: 2,3,4; group3: 3,4,5...\n",
    "buffer_size is the group size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentences(sentences, buffer_size=1):\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = ''\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j]['sentence'] + ' '\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i]['sentence']\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += ' ' + sentences[j]['sentence']\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        # Store the combined sentence in the current sentence dict\n",
    "        sentences[i]['combined_sentence'] = combined_sentence\n",
    "\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = combine_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"aspire/acge_text_embedding\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        show_progress=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding.embed_documents([x['combined_sentence'] for x in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    sentence['combined_sentence_embedding'] = embeddings[i]\n",
    "print(len(sentences[0].get(\"combined_sentence_embedding\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distances(sentences):\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
    "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "        \n",
    "        # Convert to cosine distance, 0 means the sentences are identical, 1 means they are unrelated, 2 means they are opposite\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i]['distance_to_next'] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, sentences = calculate_cosine_distances(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(distances)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # 设置宽度为10英寸，高度为5英寸\n",
    "\n",
    "# 绘制数据\n",
    "ax.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))  # 设置宽度为10英寸，高度为5英寸\n",
    "ax.plot(distances)\n",
    "# plt.plot(distances)\n",
    "\n",
    "y_upper_bound = .3\n",
    "plt.ylim(0, y_upper_bound)\n",
    "plt.xlim(0, len(distances))\n",
    "\n",
    "# We need to get the distance threshold that we'll consider an outlier\n",
    "# We'll use numpy .percentile() for this\n",
    "breakpoint_percentile_threshold = 90\n",
    "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
    "print(f\"95% threshold: {breakpoint_distance_threshold}\")\n",
    "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-')\n",
    "\n",
    "# Then we'll see how many distances are actually above this one\n",
    "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
    "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\")\n",
    "\n",
    "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
    "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
    "\n",
    "# Start of the shading and text\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
    "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
    "    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)\n",
    "\n",
    "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
    "    plt.text(x=np.average([start_index, end_index]),\n",
    "             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "             s=f\"Chunk #{i}\", horizontalalignment='center',\n",
    "             rotation='vertical')\n",
    "\n",
    "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
    "if indices_above_thresh:\n",
    "    last_breakpoint = indices_above_thresh[-1]\n",
    "    if last_breakpoint < len(distances):\n",
    "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
    "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
    "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
    "                 s=f\"Chunk #{i+1}\",\n",
    "                 rotation='vertical')\n",
    "\n",
    "plt.title(\"PG Essay Chunks Based On Embedding Breakpoints\")\n",
    "plt.xlabel(\"Index of sentences in essay (Sentence Position)\")\n",
    "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking by breakpoint_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# Create a list to hold the grouped sentences\n",
    "chunks = []\n",
    "\n",
    "# Iterate through the breakpoints to slice the sentences\n",
    "for index in indices_above_thresh:\n",
    "    # The end index is the current breakpoint\n",
    "    end_index = index\n",
    "\n",
    "    # Slice the sentence_dicts from the current start index to the end index\n",
    "    group = sentences[start_index:end_index + 1]\n",
    "    combined_text = ' '.join([d['sentence'] for d in group])\n",
    "    chunks.append(combined_text)\n",
    "    \n",
    "    # Update the start index for the next group\n",
    "    start_index = index + 1\n",
    "\n",
    "# The last group, if any sentences remain\n",
    "if start_index < len(sentences):\n",
    "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
    "    chunks.append(combined_text)\n",
    "\n",
    "# grouped_sentences now contains the chunked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Max length:{max(len(chunk) for chunk in chunks)}\")\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, chunk in enumerate(chunks[:4]):\n",
    "#     buffer = 100\n",
    "    \n",
    "#     print (f\"Chunk #{i} ###############################\")\n",
    "#     print (chunk[:buffer].strip())\n",
    "#     print (\"...\")\n",
    "#     print (chunk[-buffer:].strip())\n",
    "#     print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print (f\"Chunk #{i} ########################################################\")\n",
    "    print (chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
