{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "1. [LangChain 怎麼玩？ 動態修改運作中的 Chain 設定 / configure chain internals at runtime](https://myapollo.com.tw/blog/langchain-configure-chain-at-runtime/)\n",
    "\n",
    "2. [LangChain doc](https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/)\n",
    "\n",
    "在LangChain中可以用`configurable_fields`以及`configurable_alternatives`，動態修改chain的設定，或者置換chain上的某個Runnable(prompt, model, parser...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "llm = Ollama(model='llama3.1', temperature=0)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"如果我們想動態設定一些隨機性的話，雖然可以每次產生回應都重新載入語言模型並重新設定 temperature 參數，但是也會導致回應時間拉長，因為語言模型的載入會需要一點點時間，也許比較好的方式是讓 chain 具有動態修改設定的能力；又或者我們想做些實驗比較不同 prompt / 參數 / 模型的差異，這時候也需要動態修改設定的能力，讓我們在呼叫 chain 時使用不同的參數，以對結果進行比較；甚至是讓 chain 具有服務多名使用者的能力，針對使用者帳號不同載入不同的語言模型、對話紀錄，就像 ChatGPT 一樣，所有使用者之間不會看到彼此的對話紀錄以確保隱私，而且付費使用者多了可以切換至更強大的語言模型的功能。\n",
    "\n",
    "針對這些需求， LangChain 為每個 Runnable 提供 1 個稱為 configurable_fields 的方法，讓我們可以動態修改 chain 上的設定值。\"\n",
    "\n",
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamma3.1\n",
      "lamma3.1\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "model = Ollama(model=\"lamma3.1\", temperature=0).configurable_fields(\n",
    "    model=ConfigurableField(\n",
    "        id=\"model\",\n",
    "        name=\"Model\",\n",
    "        description=\"The model to use for the chat\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model.model)\n",
    "\n",
    "model_m = model.with_config(configurable={\"model\": \"mistral\"})\n",
    "\n",
    "# QUESTION: why\n",
    "print(model_m.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, here are three jokes for you:\\n\\n1. **Why don't scientists trust atoms?**\\n   Because they make up everything!\\n\\n2. **Why did the scarecrow win an award?**\\n   Because he was outstanding in his field!\\n\\n3. **Why don't skeletons fight each other?**\\n   They don't have the guts!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 12, 'total_tokens': 79}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None} id='run-9d0c0eeb-709c-4dc9-bf21-cbbd22e2561c-0' usage_metadata={'input_tokens': 12, 'output_tokens': 67, 'total_tokens': 79}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "# from langchain_community.llms.ollama import Ollama\n",
    "# llm = Ollama(model='llama3.1', temperature=0).configurable_fields(\n",
    "#     temperature=ConfigurableField(\n",
    "#         id=\"llm_temperature\",\n",
    "#         name=\"LLM temperature\",\n",
    "#         description=\"The temperature of the LLM\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_t09 = chain.with_config(configurable = {'llm_temperature': 1.0})\n",
    "\n",
    "res = chain_t09.invoke({'input': 'Tell me a joke'})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exampl 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1\n",
      " Of course! Here's one for you: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything... except secret laboratory recipes!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = Ollama(model='llama3.1', temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    ),\n",
    "    model=ConfigurableField(\n",
    "        id=\"llm_model\",\n",
    "        name=\"LLM model\",\n",
    "        description=\"The model of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt  = ChatPromptTemplate.from_messages([\n",
    "    ('user', '{input}'),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_t09 = chain.with_config(configurable = {'llm_temperature': 1.0, 'llm_model': 'mistral'})\n",
    "\n",
    "res = chain_t09.invoke({'input': 'Tell me a joke'})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 configurable_alternatives 動態置換 Runnable \n",
    "先前的範例都是使用 LLaMA 系列的語言模型，這些系列的語言模型都可以使用 Ollama 載入，所以可以使用 configurable_fields() 動態設定語言模型，不過 OpenAI 的語言模型就不能使用 Ollama 載入，這使得我們無法使用 configurable_fields() 方法動態切換語言模型。\n",
    "\n",
    "LangChain 還有 1 個稱為 configurable_alternatives() 的方法，可以動態置換 Runnable, 也就是說我們可以透過 configurable_alternatives() 方法，動態把 Ollama(model='llama2') 換成 ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), 其範例如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, here's a light-hearted joke for you:\\n\\nWhy don't skeletons fight each other?\\n\\nThey don't have the guts!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 11, 'total_tokens': 35}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None} id='run-23d4ce30-595f-4e4c-9282-40b545ceb6a6-0' usage_metadata={'input_tokens': 11, 'output_tokens': 24, 'total_tokens': 35}\n",
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = Ollama(model='llama3.1').configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key='llama3.1',\n",
    "    gpt4o=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.with_config(configurable={\"llm\": \"gpt4o\"}).invoke({'input': 'Tell me a joke'}))\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, here's one for you:\\n\\nWhy don't skeletons fight each other?\\n\\nThey don't have the guts!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 11, 'total_tokens': 32}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None} id='run-7b3a5150-c7fd-497a-abe2-9e0cb9792d45-0' usage_metadata={'input_tokens': 11, 'output_tokens': 21, 'total_tokens': 32}\n",
      "content='Why did the scarecrow win an award? \\n\\nBecause he was outstanding in his field!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None} id='run-741a7b28-c0f0-42fc-a949-22c7602e757c-0' usage_metadata={'input_tokens': 11, 'output_tokens': 18, 'total_tokens': 29}\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms.ollama import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini').configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key='gpt-4o-mini',\n",
    "    gpt4o=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.with_config(configurable={\"llm\": \"gpt4o\"}).invoke({'input': 'Tell me a joke'}))\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的examples中，利用`configurable_alternatives`方法設定`ConfigurableField`。\n",
    "\n",
    "然候再用`.with_config`切換`Runnable`。\n",
    "\n",
    "上面的examples是切換模型，而我們也可以切換prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, here's a joke about Earth for you:\\n\\nWhy did the Earth break up with the other planets?\\n\\nBecause they all had too much space!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 14, 'total_tokens': 43}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None} id='run-fd292406-117a-4007-8c08-f8012aeab6f5-0' usage_metadata={'input_tokens': 14, 'output_tokens': 29, 'total_tokens': 43} \n",
      "============================\n",
      "\n",
      "content=\"In the cradle of the cosmos, blue and green,\\nA jewel spins, in stardust sheen,\\nEarth, our home, where life does teem,\\nA living, breathing, ancient dream.\\n\\nMountains rise with majesty,\\nValleys whisper history,\\nOceans dance in rhythmic grace,\\nReflecting skies, a vast embrace.\\n\\nForests hum with secrets old,\\nLeaves of emerald, stories told,\\nRivers carve their timeless path,\\nIn nature's art, no aftermath.\\n\\nDeserts stretch in golden hues,\\nUnderneath the sapphire blues,\\nWinds that sing through dunes and stone,\\nA symphony of the unknown.\\n\\nCreatures great and creatures small,\\nIn this web, they heed the call,\\nFrom the eagle's lofty flight,\\nTo the firefly's gentle light.\\n\\nSeasons turn in endless flow,\\nWinter's chill and summer's glow,\\nAutumn's blaze and spring's rebirth,\\nCycles of our cherished Earth.\\n\\nYet, we tread with heavy feet,\\nOn this ground, so pure, so sweet,\\nGuardians of this fragile sphere,\\nMust we act with love and care.\\n\\nFor in the soil, the air, the sea,\\nLies the heart of you and me,\\nBound by threads of life and lore,\\nEarth, our home forevermore.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 255, 'prompt_tokens': 12, 'total_tokens': 267}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None} id='run-6a6a4489-a97f-4072-8382-d5c79ab3c7be-0' usage_metadata={'input_tokens': 12, 'output_tokens': 255, 'total_tokens': 267} \n",
      "============================\n",
      "\n",
      "content=\"(Verse 1)\\nIn the morning light, the world awakes,\\nMountains high and crystal lakes,\\nFrom the deserts dry to the ocean's blue,\\nEvery inch of Earth, a dream come true.\\n\\n(Pre-Chorus)\\nFeel the wind, hear the trees,\\nNature's song in the gentle breeze,\\nEvery heartbeat, every sigh,\\nA symphony beneath the sky.\\n\\n(Chorus)\\nOh, Earth, you're a wonderland,\\nWith your rivers and your golden sands,\\nFrom the valleys deep to the skies above,\\nYou're the home we cherish, the world we love.\\n\\n(Verse 2)\\nIn the twilight glow, the stars appear,\\nWhispering tales of yesteryear,\\nFrom the ancient woods to the coral reefs,\\nEvery story told, a world belief.\\n\\n(Pre-Chorus)\\nFeel the rain, taste the air,\\nNature's touch is everywhere,\\nEvery sunrise, every night,\\nA canvas painted with pure delight.\\n\\n(Chorus)\\nOh, Earth, you're a wonderland,\\nWith your rivers and your golden sands,\\nFrom the valleys deep to the skies above,\\nYou're the home we cherish, the world we love.\\n\\n(Bridge)\\nLet's protect this gift we've been given,\\nFor every child, for every living,\\nHand in hand, we'll stand as one,\\nGuardians of Earth, until our days are done.\\n\\n(Chorus)\\nOh, Earth, you're a wonderland,\\nWith your rivers and your golden sands,\\nFrom the valleys deep to the skies above,\\nYou're the home we cherish, the world we love.\\n\\n(Outro)\\nIn the moonlight's glow, we'll find our way,\\nCherishing Earth, day by day,\\nWith every step, with every breath,\\nWe'll honor you, our planet's depth.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 351, 'prompt_tokens': 12, 'total_tokens': 363}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_992d1ea92d', 'finish_reason': 'stop', 'logprobs': None} id='run-70cdff49-00b6-4e78-9c3e-b6fa595ad10e-0' usage_metadata={'input_tokens': 12, 'output_tokens': 351, 'total_tokens': 363}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"temll me a joke about {topic}\",\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=PromptTemplate.from_template(\"Write a poem about {topic}\"),\n",
    "    lyrics=PromptTemplate.from_template(\"Write a song about {topic}\"),\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\"topic\": \"Earth\"})\n",
    "print(res, \"\\n============================\\n\")\n",
    "\n",
    "res = chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"Earth\"})\n",
    "print(res, \"\\n============================\\n\")\n",
    "\n",
    "res = chain.with_config(configurable={\"prompt\": \"lyrics\"}).invoke({\"topic\": \"Earth\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動態載入不同使用者的對話紀錄 \n",
    "我們在使用 ChatGPT 時， ChatGPT 會幫我們保留不同的對話紀錄，而且不會看到不屬於我們的對話紀錄，這也是做任何應用重要的功能—針對不同使用者載入專屬的設定、資料。\n",
    "\n",
    "學會使用 configurable_fields() 與 configurable_alternatives() 之後，我們就可以動態載入使用者的相關設定與資料。\n",
    "\n",
    "舉對話紀錄(chat history)為例， LangChain 稱此功能為 memory, 我們可以將對話紀錄存在各種 memory 中，例如檔案、資料庫等等， LangChain 提供多種整合方案可以選擇，詳細可以閱讀此文件。\n",
    "\n",
    "本文僅展示以 Python dictionary 儲存對話紀錄作為教學範例。\n",
    "\n",
    "LangChain 其實有提供 1 個 Runnable 稱為 RunnableWithMessageHistory, 這個類別其實就是使用 configurable_fields() 方法，預設讓我們可以動態設定 session_id 藉此載入不同對話紀錄。\n",
    "\n",
    "以下是使用 RunnableWithMessageHistory 的範例程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "chat001 = ChatMessageHistory()\n",
    "chat001.add_user_message('My name is Amo.')\n",
    "\n",
    "store = {\n",
    "    'chat001': chat001,\n",
    "}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # Create a new chat history if it doesn't exist\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "llm = Ollama(model='llama3.1')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a good assistant.'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{input}'),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "session_id = 'chat001'\n",
    "input_text = input('>>> ')\n",
    "while input_text.lower() != 'bye':\n",
    "    if input_text:\n",
    "        response = with_message_history.with_config(\n",
    "            configurable={'session_id': session_id}\n",
    "        ).invoke({'input': input_text})\n",
    "        print(response)\n",
    "    input_text = input('>>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
