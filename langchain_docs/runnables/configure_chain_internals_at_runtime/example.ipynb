{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref:\n",
    "1. [LangChain 怎麼玩？ 動態修改運作中的 Chain 設定 / configure chain internals at runtime](https://myapollo.com.tw/blog/langchain-configure-chain-at-runtime/)\n",
    "\n",
    "2. [LangChain doc](https://python.langchain.com/v0.1/docs/expression_language/primitives/configure/)\n",
    "\n",
    "在LangChain中可以用`configurable_fields`以及`configurable_alternatives`，動態修改chain的設定，或者置換chain上的某個Runnable(prompt, model, parser...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "llm = Ollama(model='llama3.1', temperature=0)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"如果我們想動態設定一些隨機性的話，雖然可以每次產生回應都重新載入語言模型並重新設定 temperature 參數，但是也會導致回應時間拉長，因為語言模型的載入會需要一點點時間，也許比較好的方式是讓 chain 具有動態修改設定的能力；又或者我們想做些實驗比較不同 prompt / 參數 / 模型的差異，這時候也需要動態修改設定的能力，讓我們在呼叫 chain 時使用不同的參數，以對結果進行比較；甚至是讓 chain 具有服務多名使用者的能力，針對使用者帳號不同載入不同的語言模型、對話紀錄，就像 ChatGPT 一樣，所有使用者之間不會看到彼此的對話紀錄以確保隱私，而且付費使用者多了可以切換至更強大的語言模型的功能。\n",
    "\n",
    "針對這些需求， LangChain 為每個 Runnable 提供 1 個稱為 configurable_fields 的方法，讓我們可以動態修改 chain 上的設定值。\"\n",
    "\n",
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "model = Ollama(model=\"lamma3.1\", temperature=0).configurable_fields(\n",
    "    model=ConfigurableField(\n",
    "        id=\"model\",\n",
    "        name=\"Model\",\n",
    "        description=\"The model to use for the chat\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model.model)\n",
    "\n",
    "model_m = model.with_config(configurable={\"model\": \"mistral\"})\n",
    "\n",
    "# QUESTION: why\n",
    "print(model_m.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "# from langchain_community.llms.ollama import Ollama\n",
    "# llm = Ollama(model='llama3.1', temperature=0).configurable_fields(\n",
    "#     temperature=ConfigurableField(\n",
    "#         id=\"llm_temperature\",\n",
    "#         name=\"LLM temperature\",\n",
    "#         description=\"The temperature of the LLM\",\n",
    "#     )\n",
    "# )\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_t09 = chain.with_config(configurable = {'llm_temperature': 1.0})\n",
    "\n",
    "res = chain_t09.invoke({'input': 'Tell me a joke'})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exampl 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = Ollama(model='llama3.1', temperature=0).configurable_fields(\n",
    "    temperature=ConfigurableField(\n",
    "        id=\"llm_temperature\",\n",
    "        name=\"LLM temperature\",\n",
    "        description=\"The temperature of the LLM\",\n",
    "    ),\n",
    "    model=ConfigurableField(\n",
    "        id=\"llm_model\",\n",
    "        name=\"LLM model\",\n",
    "        description=\"The model of the LLM\",\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt  = ChatPromptTemplate.from_messages([\n",
    "    ('user', '{input}'),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_t09 = chain.with_config(configurable = {'llm_temperature': 1.0, 'llm_model': 'mistral'})\n",
    "\n",
    "res = chain_t09.invoke({'input': 'Tell me a joke'})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 configurable_alternatives 動態置換 Runnable \n",
    "先前的範例都是使用 LLaMA 系列的語言模型，這些系列的語言模型都可以使用 Ollama 載入，所以可以使用 configurable_fields() 動態設定語言模型，不過 OpenAI 的語言模型就不能使用 Ollama 載入，這使得我們無法使用 configurable_fields() 方法動態切換語言模型。\n",
    "\n",
    "LangChain 還有 1 個稱為 configurable_alternatives() 的方法，可以動態置換 Runnable, 也就是說我們可以透過 configurable_alternatives() 方法，動態把 Ollama(model='llama2') 換成 ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), 其範例如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = Ollama(model='llama3.1').configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key='llama3.1',\n",
    "    gpt4o=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.with_config(configurable={\"llm\": \"gpt4o\"}).invoke({'input': 'Tell me a joke'}))\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms.ollama import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini').configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"),\n",
    "    default_key='gpt-4o-mini',\n",
    "    gpt4o=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "print(chain.with_config(configurable={\"llm\": \"gpt4o\"}).invoke({'input': 'Tell me a joke'}))\n",
    "print(chain.invoke({'input': 'Tell me a joke'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的examples中，利用`configurable_alternatives`方法設定`ConfigurableField`。\n",
    "\n",
    "然候再用`.with_config`切換`Runnable`。\n",
    "\n",
    "上面的examples是切換模型，而我們也可以切換prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"temll me a joke about {topic}\",\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"prompt\"),\n",
    "    default_key=\"joke\",\n",
    "    poem=PromptTemplate.from_template(\"Write a poem about {topic}\"),\n",
    "    lyrics=PromptTemplate.from_template(\"Write a song about {topic}\"),\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\"topic\": \"Earth\"})\n",
    "print(res, \"\\n============================\\n\")\n",
    "\n",
    "res = chain.with_config(configurable={\"prompt\": \"poem\"}).invoke({\"topic\": \"Earth\"})\n",
    "print(res, \"\\n============================\\n\")\n",
    "\n",
    "res = chain.with_config(configurable={\"prompt\": \"lyrics\"}).invoke({\"topic\": \"Earth\"})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動態載入不同使用者的對話紀錄 \n",
    "我們在使用 ChatGPT 時， ChatGPT 會幫我們保留不同的對話紀錄，而且不會看到不屬於我們的對話紀錄，這也是做任何應用重要的功能—針對不同使用者載入專屬的設定、資料。\n",
    "\n",
    "學會使用 configurable_fields() 與 configurable_alternatives() 之後，我們就可以動態載入使用者的相關設定與資料。\n",
    "\n",
    "舉對話紀錄(chat history)為例， LangChain 稱此功能為 memory, 我們可以將對話紀錄存在各種 memory 中，例如檔案、資料庫等等， LangChain 提供多種整合方案可以選擇，詳細可以閱讀此文件。\n",
    "\n",
    "本文僅展示以 Python dictionary 儲存對話紀錄作為教學範例。\n",
    "\n",
    "LangChain 其實有提供 1 個 Runnable 稱為 RunnableWithMessageHistory, 這個類別其實就是使用 configurable_fields() 方法，預設讓我們可以動態設定 session_id 藉此載入不同對話紀錄。\n",
    "\n",
    "以下是使用 RunnableWithMessageHistory 的範例程式碼："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "chat001 = ChatMessageHistory()\n",
    "chat001.add_user_message('My name is Amo.')\n",
    "\n",
    "store = {\n",
    "    'chat001': chat001,\n",
    "}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # Create a new chat history if it doesn't exist\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "llm = Ollama(model='llama3.1')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a good assistant.'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('user', '{input}'),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_chat_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "session_id = 'chat001'\n",
    "input_text = input('>>> ')\n",
    "while input_text.lower() != 'bye':\n",
    "    if input_text:\n",
    "        response = with_message_history.with_config(\n",
    "            configurable={'session_id': session_id}\n",
    "        ).invoke({'input': input_text})\n",
    "        print(response)\n",
    "    input_text = input('>>> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
