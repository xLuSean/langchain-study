{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/10-langchain-multi-query.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/08-langchain-multi-query.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ref: `https://www.youtube.com/watch?v=VFf8XJUIHnU`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2-XDGL6Oi6h4"
   },
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# LangChain Multi-Query for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi8B1fgywJzE"
   },
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  pinecone-client==3.1.0 \\\n",
    "  langchain==0.1.1 \\\n",
    "  langchain-community==0.0.13 \\\n",
    "  datasets==2.14.6 \\\n",
    "  openai==1.6.1 \\\n",
    "  tiktoken==0.5.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CPmfrdJ9_2YA"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Py-rVqx-I0"
   },
   "source": [
    "We will download an existing dataset from Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iatOGmKgz8NE"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7E6JYtb0cW7"
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for row in data:\n",
    "    doc = Document(\n",
    "        page_content=row[\"chunk\"],\n",
    "        metadata={\n",
    "            \"title\": row[\"title\"],\n",
    "            \"source\": row[\"source\"],\n",
    "            \"id\": row[\"id\"],\n",
    "            \"chunk-id\": row[\"chunk-id\"],\n",
    "            \"text\": row[\"chunk\"]\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yb540kEs_6PZ"
   },
   "source": [
    "## Embedding and Vector DB Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKEmBZMBxtd"
   },
   "source": [
    "Initialize our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ6vTiDPBznz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass(\"OpenAI API Key: \")\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IurEkeeI-IYl"
   },
   "source": [
    "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL3KFF9E9Qb_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-multi-query-demo\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A3B7dHsd6QcP"
   },
   "source": [
    "Populate our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7Yi2YGBpTWf"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thfCYHuSpW4H"
   },
   "outputs": [],
   "source": [
    "# if you want to speed things up to follow along\n",
    "#docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXVVU97C6SwT"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    i_end = min(len(docs), i+batch_size)\n",
    "    docs_batch = docs[i:i_end]\n",
    "    # get IDs\n",
    "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "    # get text and embed\n",
    "    texts = [d.page_content for d in docs_batch]\n",
    "    embeds = embed.embed_documents(texts=texts)\n",
    "    # get metadata\n",
    "    metadata = [d.metadata for d in docs_batch]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8FbngTBzAAU-"
   },
   "source": [
    "## Multi-Query with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YVVYr13n_Ot2"
   },
   "source": [
    "Now we switch across to using our populated index as a vectorstore in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ETs0emsAh-K",
    "outputId": "0b1de24b-2f9f-48a6-d8ca-bd3d6aa007e1"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW_GCB6a3_N_"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1iptBAriANrD"
   },
   "source": [
    "We initialize the `MultiQueryRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYjztBp2ANHC"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H8qZCd1TAMAn"
   },
   "source": [
    "We set logging so that we can see the queries as they're generated by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgV1eYU6FgX7"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jrjwkpJWAaAn"
   },
   "source": [
    "To query with our multi-query retriever we call the `get_relevant_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DJ4cSJXFinV",
    "outputId": "265900d1-6aa7-4d28-cbbe-e2e95b7df7b4"
   },
   "outputs": [],
   "source": [
    "question = \"tell me about llama 2?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kSu1GsFfAqCd"
   },
   "source": [
    "From this we get a variety of docs retrieved by each of our queries independently. By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return `6` unique docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce5WBh6MFltP",
    "outputId": "f7b06949-e2a6-472e-eaf9-e712dc4bcca2"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KLMwfZPfBF89"
   },
   "source": [
    "## Adding the Generation in RAG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X79eNNL_BM4G"
   },
   "source": [
    "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNnXYOtqypiz"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "h6GVEZkhytdM",
    "outputId": "f03086b8-8d30-4d6e-a723-833ffecbcf8e"
   },
   "outputs": [],
   "source": [
    "out = qa_chain(\n",
    "    inputs={\n",
    "        \"query\": question,\n",
    "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
    "    }\n",
    ")\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KemgDCg8DkgE"
   },
   "source": [
    "## Chaining Everything with a SequentialChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kTbLlWgEEII1"
   },
   "source": [
    "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the _retrieval_ component within a `TransformChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpFmiRtYDpHp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SoD45Au1Eg-r"
   },
   "source": [
    "Now we chain this with our generation step using the `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azqCwDwXEkDT"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xpB2aWV4ESzf"
   },
   "source": [
    "Then we perform the full RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "JvJbUaLqFRG2",
    "outputId": "582caa21-777a-4a01-a618-9db64185ad5e"
   },
   "outputs": [],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bLmv01geK-ZS"
   },
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vAZVPhHzLDQQ"
   },
   "source": [
    "## Custom Multiquery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rI-KVO6zjJZw"
   },
   "source": [
    "We'll try this with two prompts, both encourage more variety in search queries.\n",
    "\n",
    "**Prompt A**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives.\n",
    "Each query MUST tackle the question from a different viewpoint,\n",
    "we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```\n",
    "\n",
    "\n",
    "**Prompt B**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IlEnYeKLFzh"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CgduNJWLBez",
    "outputId": "7ffee6c2-27b4-4bdf-8c79-7effd27e3cd4"
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "docs = retriever.get_relevant_documents(\n",
    "    query=question\n",
    ")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSySsaDKMK1i",
    "outputId": "e6f95abd-99fc-4576-d1f4-5fd4c21c70ab"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F4q65OEiizU2"
   },
   "source": [
    "Putting this together in another `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTRjTIKzi2-g"
   },
   "outputs": [],
   "source": [
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Rda74xhpjE6A"
   },
   "source": [
    "And asking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UcBY71cjGgX"
   },
   "outputs": [],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8jULksgk7gLA"
   },
   "source": [
    "After finishing, delete your Pinecone index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
