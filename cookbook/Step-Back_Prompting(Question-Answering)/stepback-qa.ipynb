{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ef724e",
   "metadata": {},
   "source": [
    "# Step-Back Prompting (Question-Answering)\n",
    "\n",
    "One prompting technique called \"Step-Back\" prompting can improve performance on complex questions by first asking a \"step back\" question. This can be combined with regular question-answering applications by then doing retrieval on both the original and step-back question.\n",
    "\n",
    "Read the paper [here](https://arxiv.org/abs/2310.06117)\n",
    "\n",
    "See an excellent blog post on this by Cobus Greyling [here](https://cobusgreyling.medium.com/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb)\n",
    "\n",
    "In this cookbook we will replicate this technique. We modify the prompts used slightly to work better with chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e017c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206415ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"was chatgpt around while trump was president?\"\n",
    "question_gen.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32667424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper(max_results=4)\n",
    "\n",
    "\n",
    "def retriever(query):\n",
    "    return search.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc28c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c77443",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever(question_gen.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "# response_prompt = ChatPromptTemplate.from_template(response_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "response_prompt = hub.pull(\"langchain-ai/stepback-answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a643ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(response_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": question_gen,\n",
    "        # \"step_back_context\": RunnableLambda(lambda x: x[\"question\"])| question_gen | retriever,\n",
    "\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "test_chain = (\n",
    "    # {\n",
    "    #     # Retrieve context using the normal question\n",
    "    #     \"normal_context\": RunnableLambda(lambda x: x[\"question\"]),\n",
    "\n",
    "    #     # Retrieve context using the step-back question\n",
    "    #     # \"step_back_context\": question_gen | retriever,\n",
    "    #     \"step_back_context\": RunnableLambda(lambda x: x[\"question\"])| question_gen,\n",
    "\n",
    "    #     # Pass on the question\n",
    "    #     \"question\": lambda x: x[\"question\"],\n",
    "    # }\n",
    "# =====================================================================\n",
    "    {'normal_context': RunnablePassthrough(),\n",
    "     'step_back_context': RunnablePassthrough() | question_gen,\n",
    "     'question': RunnablePassthrough()\n",
    "     }\n",
    "\n",
    "    | response_prompt\n",
    ")\n",
    "test_chain.invoke({\"question\": question})\n",
    "rich.print(test_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce554cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fb8dd2",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "{normal_context}\n",
    "\n",
    "Original Question: {question}\n",
    "Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06335ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question (only the first 3 results)\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9e5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
