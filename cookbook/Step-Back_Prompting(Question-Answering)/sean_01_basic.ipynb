{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://github.com/sakunaharinda/ragatouille-book/blob/main/book/2_Query_Transformation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('human', '{input}'), \n",
    "            ('ai', '{output}')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "rich.print(few_shot_prompt.format())\n",
    "rich.print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"),\n",
    "        few_shot_prompt,\n",
    "        ('user', '{question}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(final_prompt.format(question= \"What need to consider when using LLM to eval LLM generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are few ways you can write the chain\n",
    "##### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_query_chain = (\n",
    "    final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "step_back_query_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_query_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "step_back_query_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們需要用step_back_prompt找回的文件，以及原本query的文件來做出最後的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# Either syntax 1 or syntax 2 works, since only one input \"question\" is passed to the chain\n",
    "# Syntax 1\n",
    "# step_back_and_response_chain = ({\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "#      \"step_back_context\": RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "#      \"question\": RunnablePassthrough()}\n",
    "#      | response_prompt\n",
    "#      | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "#      | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# Syntax 2\n",
    "step_back_and_response_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | {\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "     \"step_back_context\": RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "     | response_prompt\n",
    "     | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = step_back_and_response_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
