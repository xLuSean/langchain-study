{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://github.com/sakunaharinda/ragatouille-book/blob/main/book/2_Query_Transformation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('human', '{input}'), \n",
    "            ('ai', '{output}')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> and the \n",
       "volume is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span> and Nov <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span>?\n",
       "AI: What was Estella Leopold's education history?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of \u001b[1;36m2\u001b[0m and the \n",
       "volume is increased by a factor of \u001b[1;36m8\u001b[0m?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug \u001b[1;36m1954\u001b[0m and Nov \u001b[1;36m1954\u001b[0m?\n",
       "AI: What was Estella Leopold's education history?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FewShotChatMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">examples</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of 2 and the volume is increased by a factor of 8?'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are the physics principles behind this question?'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Estella Leopold went to which school between Aug 1954 and Nov 1954?'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"What was Estella Leopold's education history?\"</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">example_prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{input}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'output'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{output}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mFewShotChatMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mexamples\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'input'\u001b[0m: \u001b[32m'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor \u001b[0m\n",
       "\u001b[32mof 2 and the volume is increased by a factor of 8?'\u001b[0m,\n",
       "            \u001b[32m'output'\u001b[0m: \u001b[32m'What are the physics principles behind this question?'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'input'\u001b[0m: \u001b[32m'Estella Leopold went to which school between Aug 1954 and Nov 1954?'\u001b[0m,\n",
       "            \u001b[32m'output'\u001b[0m: \u001b[32m\"What was Estella Leopold's education history?\"\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexample_prompt\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m, \u001b[32m'output'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'output'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "rich.print(few_shot_prompt.format())\n",
    "rich.print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"),\n",
    "        few_shot_prompt,\n",
    "        ('user', '{question}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic\n",
       "step-back question, which is easier to answer. Here are a few examples:\n",
       "Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> and the \n",
       "volume is increased by a factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span> and Nov <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1954</span>?\n",
       "AI: What was Estella Leopold's education history?\n",
       "Human: What need to consider when using LLM to eval LLM generation?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "System: You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic\n",
       "step-back question, which is easier to answer. Here are a few examples:\n",
       "Human: What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of \u001b[1;36m2\u001b[0m and the \n",
       "volume is increased by a factor of \u001b[1;36m8\u001b[0m?\n",
       "AI: What are the physics principles behind this question?\n",
       "Human: Estella Leopold went to which school between Aug \u001b[1;36m1954\u001b[0m and Nov \u001b[1;36m1954\u001b[0m?\n",
       "AI: What was Estella Leopold's education history?\n",
       "Human: What need to consider when using LLM to eval LLM generation?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(final_prompt.format(question= \"What need to consider when using LLM to eval LLM generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are few ways you can write the chain\n",
    "##### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What factors should be taken into account when evaluating the output of language models?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_query_chain = (\n",
    "    final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "step_back_query_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What factors should be taken into account when evaluating the output of language models?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_query_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.8)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "step_back_query_chain.invoke({\"question\": \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們需要用step_back_prompt找回的文件，以及原本query的文件來做出最後的回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean_liu/miniconda3/envs/langchain/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/sean_liu/miniconda3/envs/langchain/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# Either syntax 1 or syntax 2 works, since only one input \"question\" is passed to the chain\n",
    "# Syntax 1\n",
    "# step_back_and_response_chain = ({\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "#      \"step_back_context\": RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "#      \"question\": RunnablePassthrough()}\n",
    "#      | response_prompt\n",
    "#      | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "#      | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# Syntax 2\n",
    "step_back_and_response_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | {\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "     \"step_back_context\": RunnablePassthrough() | step_back_query_chain | retriever,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "     | response_prompt\n",
    "     | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = step_back_and_response_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">When using large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> to evaluate the outputs generated by other LLMs, several important \n",
       "considerations should be taken into account:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Reliability of Evaluators**: LLMs are increasingly being used as proxies for human evaluators. However, their \n",
       "reliability in this role is still under scrutiny. It is crucial to understand that LLMs can be sensitive to the \n",
       "specific textual instructions and inputs they receive, which may affect their evaluation outcomes.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Evaluation Pipeline Design**: The design of the evaluation pipeline is essential. For instance, systems like \n",
       "Vicuna utilize LLMs <span style=\"font-weight: bold\">(</span>e.g., GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span> to score and compare candidate responses while providing explanations. The \n",
       "simplicity and interpretability of such pipelines can be beneficial, but the underlying assumptions about the LLM's\n",
       "evaluative capabilities must be critically assessed.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Bias and Fairness**: LLMs may exhibit biases based on the data they were trained on. This can lead to unfair \n",
       "evaluations, especially if the evaluation criteria are not carefully defined or if the LLMs are not adequately \n",
       "calibrated to recognize diverse perspectives and contexts.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Context Sensitivity**: The performance of LLMs as evaluators can vary significantly based on the context in \n",
       "which they are used. Factors such as the phrasing of prompts, the nature of the responses being evaluated, and the \n",
       "specific evaluation criteria can all influence the results.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Interpretability of Results**: While LLMs can provide explanations for their evaluations, the interpretability\n",
       "of these explanations is critical. Users should be cautious about relying on LLM-generated evaluations without a \n",
       "clear understanding of how those evaluations were derived.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **Comparative Evaluation**: It may be beneficial to compare LLM evaluations with human evaluations or other \n",
       "established metrics to gauge their effectiveness and reliability. This can help identify any discrepancies and \n",
       "improve the evaluation process.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. **Continuous Improvement**: As LLMs evolve, so too should the methods used for evaluation. Regular updates and \n",
       "refinements to the evaluation criteria and processes are necessary to ensure that they remain relevant and \n",
       "effective.\n",
       "\n",
       "By considering these factors, users can better navigate the complexities involved in using LLMs for evaluating \n",
       "LLM-generated content, ultimately leading to more reliable and fair assessments.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "When using large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m to evaluate the outputs generated by other LLMs, several important \n",
       "considerations should be taken into account:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Reliability of Evaluators**: LLMs are increasingly being used as proxies for human evaluators. However, their \n",
       "reliability in this role is still under scrutiny. It is crucial to understand that LLMs can be sensitive to the \n",
       "specific textual instructions and inputs they receive, which may affect their evaluation outcomes.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Evaluation Pipeline Design**: The design of the evaluation pipeline is essential. For instance, systems like \n",
       "Vicuna utilize LLMs \u001b[1m(\u001b[0me.g., GPT-\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m to score and compare candidate responses while providing explanations. The \n",
       "simplicity and interpretability of such pipelines can be beneficial, but the underlying assumptions about the LLM's\n",
       "evaluative capabilities must be critically assessed.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Bias and Fairness**: LLMs may exhibit biases based on the data they were trained on. This can lead to unfair \n",
       "evaluations, especially if the evaluation criteria are not carefully defined or if the LLMs are not adequately \n",
       "calibrated to recognize diverse perspectives and contexts.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Context Sensitivity**: The performance of LLMs as evaluators can vary significantly based on the context in \n",
       "which they are used. Factors such as the phrasing of prompts, the nature of the responses being evaluated, and the \n",
       "specific evaluation criteria can all influence the results.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Interpretability of Results**: While LLMs can provide explanations for their evaluations, the interpretability\n",
       "of these explanations is critical. Users should be cautious about relying on LLM-generated evaluations without a \n",
       "clear understanding of how those evaluations were derived.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **Comparative Evaluation**: It may be beneficial to compare LLM evaluations with human evaluations or other \n",
       "established metrics to gauge their effectiveness and reliability. This can help identify any discrepancies and \n",
       "improve the evaluation process.\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. **Continuous Improvement**: As LLMs evolve, so too should the methods used for evaluation. Regular updates and \n",
       "refinements to the evaluation criteria and processes are necessary to ensure that they remain relevant and \n",
       "effective.\n",
       "\n",
       "By considering these factors, users can better navigate the complexities involved in using LLMs for evaluating \n",
       "LLM-generated content, ultimately leading to more reliable and fair assessments.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
