{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "The idea is generate multiple step back for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('human', '{input}'), \n",
    "            ('ai', '{output}')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FewShotChatMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">examples</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of 2 and the volume is increased by a factor of 8?'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What are the physics principles behind this question?'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Estella Leopold went to which school between Aug 1954 and Nov 1954?'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"What was Estella Leopold's education history?\"</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">example_prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">messages</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{input}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AIMessagePromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">prompt</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTemplate</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_variables</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'output'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">input_types</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">partial_variables</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{output}'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">additional_kwargs</span>=<span style=\"font-weight: bold\">{}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mFewShotChatMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mexamples\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'input'\u001b[0m: \u001b[32m'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor \u001b[0m\n",
       "\u001b[32mof 2 and the volume is increased by a factor of 8?'\u001b[0m,\n",
       "            \u001b[32m'output'\u001b[0m: \u001b[32m'What are the physics principles behind this question?'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'input'\u001b[0m: \u001b[32m'Estella Leopold went to which school between Aug 1954 and Nov 1954?'\u001b[0m,\n",
       "            \u001b[32m'output'\u001b[0m: \u001b[32m\"What was Estella Leopold's education history?\"\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexample_prompt\u001b[0m=\u001b[1;35mChatPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m, \u001b[32m'output'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmessages\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mHumanMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'input'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32minput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mAIMessagePromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mprompt\u001b[0m=\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'output'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mtemplate\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33madditional_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rich.print(few_shot_prompt.format())\n",
    "rich.print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Step_Back(BaseModel):\n",
    "    queries: List[str] = Field(description=\"step back and paraphrase of the original query, The number of step back questions is depend on the complexity of the original question, range from 1 to 5.\")\n",
    "\n",
    "multi_step_back_parser = JsonOutputParser(pydantic_object=Multi_Step_Back)\n",
    "multi_step_back_formater = multi_step_back_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate(\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to more generic step-back questions, which is easier to answer.\n",
    "\n",
    "        The number of step back questions is depend on the complexity of the original question, range from 1 to 5.\n",
    "        If the question need multiple steps of thinking, it should have more step back queries.\n",
    "        If the question is simple, it can have just one, it should have more step back queries.\n",
    "         \n",
    "         {format_instructions}\n",
    "         \n",
    "         Here are a few examples:\n",
    "         {few_shot_examples}\n",
    "         \"\"\",\n",
    "         partial_variables={\n",
    "             'format_instructions': multi_step_back_formater,\n",
    "                'few_shot_examples': few_shot_prompt.format()\n",
    "             }\n",
    "    )\n",
    ")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template='{question}',\n",
    "        input_variables=['question']\n",
    "    )\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        # few_shot_prompt,\n",
    "        human_message\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Query generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_generator = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9)\n",
    "    | multi_step_back_parser\n",
    "    | (lambda x: x['queries'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_generator.invoke(\"What need to consider when using LLM to eval LLM generation?\")\n",
    "# test = multi_step_back_queries_generator.invoke(\"How to pick rock from floor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'What are the factors to consider when evaluating language model outputs?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'How do different criteria affect the evaluation of language models?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'What aspects of language model generation should be critically analyzed?'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'What are the best practices for assessing language model performance?'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'What are the factors to consider when evaluating language model outputs?'\u001b[0m,\n",
       "    \u001b[32m'How do different criteria affect the evaluation of language models?'\u001b[0m,\n",
       "    \u001b[32m'What aspects of language model generation should be critically analyzed?'\u001b[0m,\n",
       "    \u001b[32m'What are the best practices for assessing language model performance?'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(test))\n",
    "rich.print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sean_liu/miniconda3/envs/langchain/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/sean_liu/miniconda3/envs/langchain/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/fxpzwr797mz7hy09lrlmt2500000gn/T/ipykernel_31150/2694125691.py:12: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add retriever into queries generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_chain = (\n",
    "    multi_step_back_queries_generator\n",
    "    | retriever.map()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'prompt the model to generate evaluation evidence\\nbefore assigning scores, leveraging the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">inherent\\nproperties of causal language models for calibra-\\ntion. We also employ ensemble techniques to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">incor-\\nporate multiple evidence calibration results to fur-\\nther stabilize the evaluation. 2) Balanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Position\\nCalibration (BPC): To further reduce positional\\nbias, we evaluate each candidate in both </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">positions\\nacross two runs and compute the final score as the\\naverage of the two runs. 3) Human In The Loop'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), 12067–12097. Toronto, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Canada:\\nAssociation for Computational Linguistics.'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Inference. In Proceedings of the 57th Annual Meet-\\ning of the Association for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Computational Linguistics\\n(ACL).\\nBowman, S. R. 2023. Eight things to know about large\\nlanguage models. arXiv </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">preprint arXiv:2304.00612.\\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\\nplan, J.; Dhariwal, P.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Neelakantan, A.; Shyam, P.;\\nSastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\\nA.; Krueger, G.; Henighan, T.; </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Child, R.; Ramesh,\\nA.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ample, The evaluation pipeline of Vicuna (Zheng\\net al., 2023) has gained significant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">candidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs (Dong </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  [cs.CL]  30 Aug 2023'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ample, The evaluation pipeline of Vicuna (Zheng\\net al., 2023) has gained significant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">candidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs (Dong </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  [cs.CL]  30 Aug 2023'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ample, The evaluation pipeline of Vicuna (Zheng\\net al., 2023) has gained significant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">candidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">how reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs (Dong </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  [cs.CL]  30 Aug 2023'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../../pdf_files/Large Language Models are not Fair Evaluators(2305.17926v2).pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'with human intent (He et al., 2023). While hu-\\nman evaluation is treated as the most </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">accurate mea-\\nsurement of model performance, it is costly and\\ntime-consuming to operate at scales. Consider-\\ning</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the potent capabilities of LLMs, researchers\\nhave started utilizing LLMs to evaluate the profi-\\nciency of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">generative models in adhering to human\\ninstructions (Zheng et al., 2023; Lu et al., 2023; Li\\net al., 2023). In </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">these works, Vicuna’s evaluation\\nparadigm (Zheng et al., 2023) is widely adopted,'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'prompt the model to generate evaluation evidence\\nbefore assigning scores, leveraging the\u001b[0m\n",
       "\u001b[32minherent\\nproperties of causal language models for calibra-\\ntion. We also employ ensemble techniques to \u001b[0m\n",
       "\u001b[32mincor-\\nporate multiple evidence calibration results to fur-\\nther stabilize the evaluation. 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Balanced \u001b[0m\n",
       "\u001b[32mPosition\\nCalibration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBPC\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: To further reduce positional\\nbias, we evaluate each candidate in both \u001b[0m\n",
       "\u001b[32mpositions\\nacross two runs and compute the final score as the\\naverage of the two runs. 3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Human In The Loop'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Model-Based Evaluation Metrics for Text Generation.\\nIn Proceedings of the 61st Annual \u001b[0m\n",
       "\u001b[32mMeeting of the\\nAssociation for Computational Linguistics \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVolume\\n1: Long Papers\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 12067–12097. Toronto, \u001b[0m\n",
       "\u001b[32mCanada:\\nAssociation for Computational Linguistics.'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'Inference. In Proceedings of the 57th Annual Meet-\\ning of the Association for \u001b[0m\n",
       "\u001b[32mComputational Linguistics\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mACL\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nBowman, S. R. 2023. Eight things to know about large\\nlanguage models. arXiv \u001b[0m\n",
       "\u001b[32mpreprint arXiv:2304.00612.\\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\\nplan, J.; Dhariwal, P.; \u001b[0m\n",
       "\u001b[32mNeelakantan, A.; Shyam, P.;\\nSastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\\nA.; Krueger, G.; Henighan, T.; \u001b[0m\n",
       "\u001b[32mChild, R.; Ramesh,\\nA.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'ample, The evaluation pipeline of Vicuna \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has gained significant \u001b[0m\n",
       "\u001b[32minterest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare \u001b[0m\n",
       "\u001b[32mcandidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear\u001b[0m\n",
       "\u001b[32mhow reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDong \u001b[0m\n",
       "\u001b[32met al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  30 Aug 2023'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'ample, The evaluation pipeline of Vicuna \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has gained significant \u001b[0m\n",
       "\u001b[32minterest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare \u001b[0m\n",
       "\u001b[32mcandidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear\u001b[0m\n",
       "\u001b[32mhow reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDong \u001b[0m\n",
       "\u001b[32met al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  30 Aug 2023'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'ample, The evaluation pipeline of Vicuna \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has gained significant \u001b[0m\n",
       "\u001b[32minterest and wide\\nusage due to its simplicity and interpretability. It\\nprompts GPT-4 to score and compare \u001b[0m\n",
       "\u001b[32mcandidate\\nresponses and provide explanations, making it a\\nvaluable tool for evaluation. However, it is un-\\nclear\u001b[0m\n",
       "\u001b[32mhow reliable LLMs are as evaluators, as they\\nare known to be sensitive to textual instructions\\nand inputs \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDong \u001b[0m\n",
       "\u001b[32met al., 2022; Turpin et al., 2023;\\narXiv:2305.17926v2  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.CL\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  30 Aug 2023'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                \u001b[32m'page'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                \u001b[32m'source'\u001b[0m: \u001b[32m'../../pdf_files/Large Language Models are not Fair Evaluators\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2305.17926v2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[33mpage_content\u001b[0m=\u001b[32m'with human intent \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHe et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. While hu-\\nman evaluation is treated as the most \u001b[0m\n",
       "\u001b[32maccurate mea-\\nsurement of model performance, it is costly and\\ntime-consuming to operate at scales. Consider-\\ning\u001b[0m\n",
       "\u001b[32mthe potent capabilities of LLMs, researchers\\nhave started utilizing LLMs to evaluate the profi-\\nciency of \u001b[0m\n",
       "\u001b[32mgenerative models in adhering to human\\ninstructions \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng et al., 2023; Lu et al., 2023; Li\\net al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In \u001b[0m\n",
       "\u001b[32mthese works, Vicuna’s evaluation\\nparadigm \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZheng et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is widely adopted,'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def rrf(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # assumes the docs are returned in the order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1/(rank+k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc_str), score) for doc_str, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_chain = (\n",
    "    multi_step_back_queries_generator\n",
    "    | retriever.map()\n",
    "    | rrf\n",
    "    | (lambda obj_list: \"\\n\".join(f\"<doc_{i}>{obj[0].page_content}</doc_{i}>\" for i, obj in enumerate(obj_list) if obj and obj[0].page_content))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc_0>Model-Based Evaluation Metrics for Text Generation.\n",
      "In Proceedings of the 61st Annual Meeting of the\n",
      "Association for Computational Linguistics (Volume\n",
      "1: Long Papers), 12067–12097. Toronto, Canada:\n",
      "Association for Computational Linguistics.</doc_0>\n",
      "<doc_1>Table 1: The evaluation template with three slots ({Q},\n",
      "{R1} and {R2}) from Zheng et al. (2023). Even though\n",
      "the template emphasizes not letting the order affect the\n",
      "results (red text), large language models still have a\n",
      "large positional bias.\n",
      "promising their fairness as evaluators; 2) We de-\n",
      "velop a calibration framework with three simple yet\n",
      "effective strategies to calibrate the positional bias of\n",
      "LLMs; 3) We manually annotate the “win/tie/lose”\n",
      "outcomes of responses from ChatGPT and Vicuna-</doc_1>\n",
      "<doc_2>Large Language Models are not Fair Evaluators\n",
      "Peiyi Wang1 Lei Li1 Liang Chen1 Zefan Cai1 Dawei Zhu1\n",
      "Binghuai Lin3 Yunbo Cao3 Qi Liu2 Tianyu Liu3 Zhifang Sui1\n",
      "1 National Key Laboratory for Multimedia Information Processing, Peking University\n",
      "2 The University of Hong Kong 3 Tencent Cloud AI\n",
      "{wangpeiyi9979, nlp.lilei, zefncai}@gmail.com\n",
      "leo.liang.chen@outlook.com; {dwzhu, szf}@pku.edu.cn\n",
      "liuqi@cs.hku.hk; {binghuailin, yunbocao, rogertyliu}@tencent.com\n",
      "Abstract</doc_2>\n",
      "<doc_3>Inference. In Proceedings of the 57th Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(ACL).\n",
      "Bowman, S. R. 2023. Eight things to know about large\n",
      "language models. arXiv preprint arXiv:2304.00612.\n",
      "Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\n",
      "plan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.;\n",
      "Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\n",
      "A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh,\n",
      "A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;</doc_3>\n",
      "<doc_4>95546-34-4.\n",
      "Liu, T.; Xin, Z.; Ding, X.; Chang, B.; and Sui, Z. 2020b.\n",
      "An Empirical Study on Model-agnostic Debiasing\n",
      "Strategies for Robust Natural Language Inference.\n",
      "In Proceedings of the 24th Conference on Computa-\n",
      "tional Natural Language Learning. Online: Associa-\n",
      "tion for Computational Linguistics.\n",
      "Lu, Q.; Qiu, B.; Ding, L.; Xie, L.; and Tao, D. 2023.\n",
      "Error analysis prompting enables human-like trans-\n",
      "lation evaluation in large language models: A case</doc_4>\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_and_response_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | {\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "     \"step_back_context\": RunnablePassthrough() | multi_step_back_queries_chain,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "     | response_prompt\n",
    "     | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = step_back_and_response_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">When using large language models <span style=\"font-weight: bold\">(</span>LLMs<span style=\"font-weight: bold\">)</span> to evaluate the generation of other LLMs, several important considerations \n",
       "should be taken into account:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. **Reliability of LLMs as Evaluators**: LLMs can be sensitive to the textual instructions and inputs they \n",
       "receive. This sensitivity raises questions about their reliability as evaluators. Variations in phrasing or context\n",
       "can lead to different evaluations, which may not accurately reflect the quality of the generated text.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. **Interpretability of Evaluation Metrics**: The evaluation pipeline, such as that used by Vicuna, is noted for \n",
       "its simplicity and interpretability. However, it is crucial to ensure that the evaluation criteria used are clear \n",
       "and understandable, allowing for consistent assessments across different evaluations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. **Human Intent Alignment**: Evaluating how well generated content aligns with human intent is essential. While \n",
       "LLMs can provide insights into adherence to instructions, they may not fully capture the nuances of human \n",
       "expectations and preferences.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. **Cost and Scalability**: Human evaluation is often seen as the gold standard for assessing model performance, \n",
       "but it is costly and time-consuming. LLMs offer a scalable alternative, but the trade-off in terms of evaluation \n",
       "accuracy must be considered.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. **Potential Biases**: LLMs may carry biases from their training data, which can affect their evaluations. It is \n",
       "important to be aware of these biases and to consider how they might influence the assessment of generated content.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>. **Comparative Evaluation**: When using LLMs to compare different candidate responses, it is essential to ensure \n",
       "that the evaluation process is fair and consistent across all candidates. This includes using the same criteria and\n",
       "prompts for all evaluations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>. **Feedback Mechanisms**: Providing explanations for evaluations can enhance the interpretability of the results.\n",
       "Understanding why an LLM rated a response in a certain way can help improve future generations and evaluations.\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>. **Continuous Improvement**: As LLMs evolve, so too should the evaluation methods. Regularly updating evaluation \n",
       "frameworks to incorporate new findings and methodologies can help maintain the relevance and accuracy of \n",
       "assessments.\n",
       "\n",
       "By considering these factors, researchers and practitioners can better utilize LLMs for evaluating text generation,\n",
       "ensuring that the evaluations are meaningful and reflective of true performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "When using large language models \u001b[1m(\u001b[0mLLMs\u001b[1m)\u001b[0m to evaluate the generation of other LLMs, several important considerations \n",
       "should be taken into account:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. **Reliability of LLMs as Evaluators**: LLMs can be sensitive to the textual instructions and inputs they \n",
       "receive. This sensitivity raises questions about their reliability as evaluators. Variations in phrasing or context\n",
       "can lead to different evaluations, which may not accurately reflect the quality of the generated text.\n",
       "\n",
       "\u001b[1;36m2\u001b[0m. **Interpretability of Evaluation Metrics**: The evaluation pipeline, such as that used by Vicuna, is noted for \n",
       "its simplicity and interpretability. However, it is crucial to ensure that the evaluation criteria used are clear \n",
       "and understandable, allowing for consistent assessments across different evaluations.\n",
       "\n",
       "\u001b[1;36m3\u001b[0m. **Human Intent Alignment**: Evaluating how well generated content aligns with human intent is essential. While \n",
       "LLMs can provide insights into adherence to instructions, they may not fully capture the nuances of human \n",
       "expectations and preferences.\n",
       "\n",
       "\u001b[1;36m4\u001b[0m. **Cost and Scalability**: Human evaluation is often seen as the gold standard for assessing model performance, \n",
       "but it is costly and time-consuming. LLMs offer a scalable alternative, but the trade-off in terms of evaluation \n",
       "accuracy must be considered.\n",
       "\n",
       "\u001b[1;36m5\u001b[0m. **Potential Biases**: LLMs may carry biases from their training data, which can affect their evaluations. It is \n",
       "important to be aware of these biases and to consider how they might influence the assessment of generated content.\n",
       "\n",
       "\u001b[1;36m6\u001b[0m. **Comparative Evaluation**: When using LLMs to compare different candidate responses, it is essential to ensure \n",
       "that the evaluation process is fair and consistent across all candidates. This includes using the same criteria and\n",
       "prompts for all evaluations.\n",
       "\n",
       "\u001b[1;36m7\u001b[0m. **Feedback Mechanisms**: Providing explanations for evaluations can enhance the interpretability of the results.\n",
       "Understanding why an LLM rated a response in a certain way can help improve future generations and evaluations.\n",
       "\n",
       "\u001b[1;36m8\u001b[0m. **Continuous Improvement**: As LLMs evolve, so too should the evaluation methods. Regularly updating evaluation \n",
       "frameworks to incorporate new findings and methodologies can help maintain the relevance and accuracy of \n",
       "assessments.\n",
       "\n",
       "By considering these factors, researchers and practitioners can better utilize LLMs for evaluating text generation,\n",
       "ensuring that the evaluations are meaningful and reflective of true performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
