{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "The idea is generate multiple step back for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        'input': 'What happens to the pressure, P, of an ideal gas if the temperature is increased by a factor of 2 and the volume is increased by a factor of 8?',\n",
    "        'output': 'What are the physics principles behind this question?'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Estella Leopold went to which school between Aug 1954 and Nov 1954?',\n",
    "        'output': \"What was Estella Leopold's education history?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('human', '{input}'), \n",
    "            ('ai', '{output}')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rich.print(few_shot_prompt.format())\n",
    "rich.print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Step_Back(BaseModel):\n",
    "    queries: List[str] = Field(description=\"step back and paraphrase of the original query, The number of step back questions is depend on the complexity of the original question, range from 1 to 5.\")\n",
    "\n",
    "multi_step_back_parser = JsonOutputParser(pydantic_object=Multi_Step_Back)\n",
    "multi_step_back_formater = multi_step_back_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate(\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to more generic step-back questions, which is easier to answer.\n",
    "\n",
    "        The number of step back questions is depend on the complexity of the original question, range from 1 to 5.\n",
    "        If the question need multiple steps of thinking, it should have more step back queries.\n",
    "        If the question is simple, it can have just one, it should have more step back queries.\n",
    "         \n",
    "         {format_instructions}\n",
    "         \n",
    "         Here are a few examples:\n",
    "         {few_shot_examples}\n",
    "         \"\"\",\n",
    "         partial_variables={\n",
    "             'format_instructions': multi_step_back_formater,\n",
    "                'few_shot_examples': few_shot_prompt.format()\n",
    "             }\n",
    "    )\n",
    ")\n",
    "\n",
    "human_message = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template='{question}',\n",
    "        input_variables=['question']\n",
    "    )\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        # few_shot_prompt,\n",
    "        human_message\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Query generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_generator = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | final_prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.9)\n",
    "    | multi_step_back_parser\n",
    "    | (lambda x: x['queries'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_generator.invoke(\"What need to consider when using LLM to eval LLM generation?\")\n",
    "# test = multi_step_back_queries_generator.invoke(\"How to pick rock from floor?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))\n",
    "rich.print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add retriever into queries generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_chain = (\n",
    "    multi_step_back_queries_generator\n",
    "    | retriever.map()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def rrf(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # assumes the docs are returned in the order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1/(rank+k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc_str), score) for doc_str, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_back_queries_chain = (\n",
    "    multi_step_back_queries_generator\n",
    "    | retriever.map()\n",
    "    | rrf\n",
    "    | (lambda obj_list: \"\\n\".join(f\"<doc_{i}>{obj[0].page_content}</doc_{i}>\" for i, obj in enumerate(obj_list) if obj and obj[0].page_content))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multi_step_back_queries_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. \n",
    "Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "<normal_context>\n",
    "# {normal_context}\n",
    "</normal_context>\n",
    "\n",
    "<step_back_context>\n",
    "# {step_back_context}\n",
    "</step_back_context>\n",
    "\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "step_back_and_response_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | {\"normal_context\": RunnablePassthrough() |  retriever,\n",
    "     \"step_back_context\": RunnablePassthrough() | multi_step_back_queries_chain,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "     | response_prompt\n",
    "     | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
    "     | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = step_back_and_response_chain.invoke(\"What need to consider when using LLM to eval LLM generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
