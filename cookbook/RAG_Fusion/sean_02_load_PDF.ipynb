{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sean's adaptation\n",
    "#### Goal:\n",
    "Use `JsonOutputParser` to get more stable output format when generate multi-queiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('../../pdf_files/',glob=\"*.pdf\",loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "\n",
    "text_splitter  = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=text_chunks, \n",
    "                                    embedding=embedding,\n",
    "                                    persist_directory=\"data/vectorstore\")\n",
    "vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Multi_Queries(BaseModel):\n",
    "    multi_queries: List[str]=Field(description=\"The new queries that rephrase user's query with different perspectives.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "multi_queries_parser = JsonOutputParser(pydantic_object=Multi_Queries)\n",
    "multi_queries_format = multi_queries_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate)\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query.\n",
    "Generate 4 queries.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "system_message = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=system_prompt,\n",
    "        # input_variables=['format_instructions']\n",
    "        partial_variables={'format_instructions': multi_queries_format}\n",
    "    )\n",
    ")\n",
    "human_message = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"Generate multiple search queries related to: {original_query}\",\n",
    "        input_variables=['original_query']\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message,\n",
    "        human_message\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "generate_multi_queries =(\n",
    "{\"original_query\": RunnablePassthrough()}\n",
    "| prompt\n",
    "| ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "| multi_queries_parser\n",
    "| (lambda x: x['multi_queries'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_multi_queries.invoke({\"original_query\": \"What need to consider when using LLM to eval LLM generation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def rrf(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # assumes the docs are returned in the order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            fused_scores[doc_str] += 1/(rank+k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc_str), {\"rrf_score\": score}) for doc_str, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    \n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrf_chain = generate_multi_queries | retriever.map() | rrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"original_query\": \"What need to consider when using LLM to eval LLM generation?\"}\n",
    "final_result = rrf_chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in final_result:\n",
    "    print(doc[1]['rrf_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def filter_rrf_score(docs, threshold=0.1):\n",
    "    return [doc for doc in docs if doc[1]['rrf_score'] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = filter_rrf_score(final_result, threshold=0.04)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_chain_test = rrf_chain | filter_rrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"original_query\": \"What need to consider when using LLM to eval LLM generation?\"}\n",
    "test = filter_chain_test.invoke({\"original_query\": input})\n",
    "# test = filter_chain_test.invoke({\"original_query\": input, \"threshold\": 0.0})\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_docs(docs):\n",
    "    documents = \"\"\n",
    "    for doc in docs:\n",
    "        documents += doc[0].page_content + \"\\n\\n\"\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_chain = rrf_chain | concatenate_docs\n",
    "input = {\"original_query\": \"What need to consider when using LLM to eval LLM generation?\"}\n",
    "\n",
    "test = concatenate_chain.invoke(input)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a helpful assistant that generates answer based on user's input query and retrieved documents.\n",
    "\n",
    "<retreived_documents>\n",
    "{retreived_documents}\n",
    "</retreived_documents>\n",
    "\n",
    "<user_query>\n",
    "{user_query}\n",
    "</user_query>\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['retreived_documents', 'user_query']\n",
    "    )\n",
    "\n",
    "respond_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "response_chain = rag_prompt | respond_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"original_query\": \"What need to consider when using LLM to eval LLM generation?\"}\n",
    "\n",
    "docs = concatenate_chain.invoke(input)\n",
    "\n",
    "rag_result = response_chain.invoke({\"retreived_documents\": docs, \"user_query\": input['original_query']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(rag_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
